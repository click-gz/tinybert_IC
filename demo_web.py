#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Web Demo for Dialogue Intent Classification using Gradio

Usage:
    python demo_web.py --checkpoint checkpoints/teacher/best_model.pt
    
Then open http://localhost:7860 in your browser
"""

import argparse
import gradio as gr
import torch
from transformers import BertTokenizer
import json
from typing import List, Dict
import pandas as pd

from src import (
    MultiTurnDialogueClassifier,
    load_config,
    get_device
)


# Intent label mapping (æ™ºèƒ½åŠ©æ‰‹æ„å›¾è¯†åˆ«)
INTENT_LABELS = {
    0: "é—®é¢˜æ±‚åŠ©/æŠ€æœ¯æ”¯æŒ",
    1: "é—²èŠ",
    2: "å¤©æ°”æŸ¥è¯¢",
    3: "æ–°é—»èµ„è®¯",
    4: "éŸ³ä¹æ’­æ”¾",
    5: "æé†’è®¾ç½®",
    6: "è®¾å¤‡çŠ¶æ€æŸ¥è¯¢",
    7: "é€šè¯è§†é¢‘",
    8: "è·Œå€’ç›‘æµ‹ç¡®è®¤",
    9: "ç©ºé—´æ“ä½œ",
    10: "è§†è§‰è¯†åˆ«",
    11: "é¤å…æ¨è"
}


class WebPredictor:
    """Webç•Œé¢é¢„æµ‹å™¨"""
    
    def __init__(self, checkpoint_path: str, config_path: str = 'config/teacher_config.yaml'):
        # åŠ è½½é…ç½®
        self.config = load_config(config_path)
        self.device = get_device(self.config.get('device', 'cuda'))
        
        # åŠ è½½tokenizer
        self.tokenizer = BertTokenizer.from_pretrained(self.config['model']['encoder_name'])
        
        # åˆ›å»ºå¹¶åŠ è½½æ¨¡å‹
        self.model = MultiTurnDialogueClassifier(
            encoder_name=self.config['model']['encoder_name'],
            num_labels=self.config['model']['num_labels'],
            max_seq_length=self.config['data']['max_seq_length'],
            dropout=self.config['model']['dropout']
        )
        
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.to(self.device)
        self.model.eval()
        
        print(f"âœ“ æ¨¡å‹åŠ è½½æˆåŠŸï¼è®¾å¤‡: {self.device}")
    
    def predict_from_turns(self, turns_text: str) -> tuple:
        """
        ä»å¤šè½®å¯¹è¯æ–‡æœ¬é¢„æµ‹æ„å›¾ï¼ˆä¸è®­ç»ƒæ—¶æ ¼å¼å®Œå…¨ä¸€è‡´ï¼‰
        
        Args:
            turns_text: å¯¹è¯æ–‡æœ¬ï¼Œæ¯è¡Œä¸€è½®ï¼Œæ ¼å¼: [è¯´è¯äºº] è¯è¯­å†…å®¹
            
        Returns:
            (é¢„æµ‹ç»“æœæ–‡æœ¬, Top-Ké¢„æµ‹è¡¨æ ¼, æ¦‚ç‡åˆ†å¸ƒå›¾)
        """
        try:
            # è§£æå¯¹è¯
            turns = []
            for line in turns_text.strip().split('\n'):
                line = line.strip()
                if not line:
                    continue
                
                # è§£æ [è¯´è¯äºº] è¯è¯­
                if line.startswith('[') and ']' in line:
                    speaker_end = line.index(']')
                    speaker = line[1:speaker_end].strip().lower()
                    text = line[speaker_end+1:].strip()
                else:
                    # é»˜è®¤ä¸ºå®¢æˆ·
                    speaker = 'user'
                    text = line
                
                # ç»Ÿä¸€è½¬æ¢ä¸º user/system
                if speaker in ['é¡¾å®¢', 'å®¢æˆ·', 'ç”¨æˆ·', 'user', 'customer']:
                    speaker = 'user'
                else:
                    speaker = 'system'
                
                turns.append({
                    'speaker': speaker,
                    'text': text
                })
            
            if not turns:
                return "âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„å¯¹è¯å†…å®¹ï¼", None, None
            
            # æ‹¼æ¥å¯¹è¯ - ä¸è®­ç»ƒæ—¶çš„dataset.pyæ ¼å¼å®Œå…¨ä¸€è‡´
            dialogue_parts = []
            for turn in turns:
                speaker = turn['speaker'].upper()  # USER or SYSTEM
                text = turn['text']
                dialogue_parts.append(f"[{speaker}] {text}")
            
            full_dialogue = " ".join(dialogue_parts)
            
            # Tokenize - ä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´
            encoded = self.tokenizer(
                full_dialogue,
                max_length=self.config['data']['max_seq_length'],
                padding='max_length',
                truncation=True,
                return_tensors='pt'
            )
            
            for key in encoded:
                encoded[key] = encoded[key].to(self.device)
            
            # é¢„æµ‹
            with torch.no_grad():
                outputs = self.model(**encoded)
                logits = outputs['logits']
                probs = torch.softmax(logits, dim=-1)
                
                predicted_id = torch.argmax(probs, dim=-1).item()
                confidence = probs[0, predicted_id].item()
                all_probs = probs[0].cpu().numpy()
            
            # æ ¼å¼åŒ–ç»“æœ
            result_text = f"""
## ğŸ¯ é¢„æµ‹ç»“æœ

**æ„å›¾**: {INTENT_LABELS.get(predicted_id, 'æœªçŸ¥')}  
**ç½®ä¿¡åº¦**: {confidence:.4f} ({confidence*100:.2f}%)  

### ğŸ“ è¾“å…¥å¯¹è¯:
"""
            for i, turn in enumerate(turns, 1):
                speaker_cn = "ç”¨æˆ·" if turn['speaker'] == 'user' else "å®¢æœ"
                result_text += f"{i}. **[{speaker_cn}]**: {turn['text']}\n"
            
            # Top-Ké¢„æµ‹è¡¨æ ¼
            top_k = min(5, len(all_probs))
            top_indices = torch.topk(torch.tensor(all_probs), k=top_k).indices.numpy()
            
            top_k_data = []
            for rank, idx in enumerate(top_indices, 1):
                top_k_data.append({
                    'æ’å': rank,
                    'æ„å›¾': INTENT_LABELS.get(int(idx), 'æœªçŸ¥'),
                    'ç½®ä¿¡åº¦': f"{all_probs[idx]:.4f}",
                    'ç™¾åˆ†æ¯”': f"{all_probs[idx]*100:.2f}%"
                })
            
            top_k_df = pd.DataFrame(top_k_data)
            
            # æ¦‚ç‡åˆ†å¸ƒå›¾
            prob_data = {
                'æ„å›¾': [INTENT_LABELS.get(i, f'Intent_{i}') for i in range(len(all_probs))],
                'æ¦‚ç‡': all_probs.tolist()
            }
            prob_df = pd.DataFrame(prob_data)
            prob_df = prob_df.sort_values('æ¦‚ç‡', ascending=False)
            
            return result_text, top_k_df, prob_df
            
        except Exception as e:
            return f"âŒ é¢„æµ‹å‡ºé”™: {str(e)}", None, None


def create_demo(predictor: WebPredictor):
    """åˆ›å»ºGradioç•Œé¢"""
    
    # ç¤ºä¾‹å¯¹è¯ï¼ˆæ™ºèƒ½åŠ©æ‰‹åœºæ™¯ï¼‰
    examples = [
        "[ç”¨æˆ·] ä½ å¥½\n[åŠ©æ‰‹] æ‚¨å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨çš„å—ï¼Ÿ\n[ç”¨æˆ·] ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·",
        "[ç”¨æˆ·] æ’­æ”¾ä¸€é¦–æ­Œ\n[åŠ©æ‰‹] å¥½çš„ï¼Œè¯·é—®æƒ³å¬ä»€ä¹ˆæ­Œï¼Ÿ\n[ç”¨æˆ·] å‘¨æ°ä¼¦çš„æ­Œ",
        "[ç”¨æˆ·] æˆ‘çš„WiFiè¿ä¸ä¸Šäº†\n[åŠ©æ‰‹] æˆ‘æ¥å¸®æ‚¨æ’æŸ¥ã€‚è¯·é—®å…¶ä»–è®¾å¤‡èƒ½è¿æ¥å—ï¼Ÿ\n[ç”¨æˆ·] å…¶ä»–è®¾å¤‡å¯ä»¥",
        "[ç”¨æˆ·] è¿˜æœ‰å¤šå°‘ç”µé‡\n[åŠ©æ‰‹] å½“å‰ç”µé‡65%ï¼Œé¢„è®¡å¯ä½¿ç”¨5å°æ—¶ã€‚\n[ç”¨æˆ·] å¼€å¯çœç”µæ¨¡å¼",
        "[ç”¨æˆ·] æ˜å¤©æ—©ä¸Š7ç‚¹æé†’æˆ‘å¼€ä¼š\n[åŠ©æ‰‹] å¥½çš„ï¼Œå·²ä¸ºæ‚¨è®¾ç½®æ˜å¤©7:00çš„æé†’ã€‚\n[ç”¨æˆ·] è°¢è°¢",
        "[ç”¨æˆ·] è¿™æ˜¯ä»€ä¹ˆä¸œè¥¿\n[åŠ©æ‰‹] æˆ‘æ¥è¯†åˆ«ä¸€ä¸‹...è¿™æ˜¯ä¸€ä¸ªé¥æ§å™¨ã€‚\n[ç”¨æˆ·] èƒ½è¯¦ç»†ä»‹ç»ä¸€ä¸‹å—",
        "[ç”¨æˆ·] å»å¨æˆ¿å¸®æˆ‘æ‹¿é¥æ§å™¨\n[åŠ©æ‰‹] å¥½çš„ï¼Œæ­£åœ¨å‰å¾€å¨æˆ¿ã€‚\n[ç”¨æˆ·] å¿«ç‚¹",
        "[ç”¨æˆ·] é™„è¿‘æœ‰ä»€ä¹ˆå¥½åƒçš„\n[åŠ©æ‰‹] æ­£åœ¨ä¸ºæ‚¨æŸ¥è¯¢é™„è¿‘é¤å…...\n[ç”¨æˆ·] è¦è¯„åˆ†é«˜çš„",
    ]
    
    with gr.Blocks(title="å¯¹è¯æ„å›¾åˆ†ç±»ç³»ç»Ÿ", theme=gr.themes.Soft()) as demo:
        gr.Markdown("""
        # ğŸ¤– æ™ºèƒ½åŠ©æ‰‹å¤šè½®å¯¹è¯æ„å›¾åˆ†ç±»ç³»ç»Ÿ
        
        ## ä½¿ç”¨è¯´æ˜
        1. åœ¨å·¦ä¾§æ–‡æœ¬æ¡†ä¸­è¾“å…¥å¤šè½®å¯¹è¯ï¼Œæ¯è¡Œä¸€è½®
        2. æ ¼å¼: `[è¯´è¯äºº] è¯è¯­å†…å®¹`ï¼Œä¾‹å¦‚ `[ç”¨æˆ·] ä½ å¥½` æˆ– `[åŠ©æ‰‹] æ‚¨å¥½`
        3. è¯´è¯äººå¯ä»¥æ˜¯: **user/ç”¨æˆ·** æˆ– **system/åŠ©æ‰‹/æœºå™¨äºº**ï¼ˆä¸­è‹±æ–‡å‡å¯ï¼‰
        4. ç‚¹å‡»"é¢„æµ‹"æŒ‰é’®æŸ¥çœ‹ç»“æœ
        5. æˆ–ç‚¹å‡»ä¸‹æ–¹ç¤ºä¾‹å¿«é€Ÿä½“éªŒ
        """)
        
        with gr.Row():
            with gr.Column(scale=1):
                input_text = gr.Textbox(
                    label="è¾“å…¥å¤šè½®å¯¹è¯",
                    placeholder="[ç”¨æˆ·] ä½ å¥½\n[åŠ©æ‰‹] æ‚¨å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨çš„å—ï¼Ÿ\n[ç”¨æˆ·] ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·",
                    lines=10,
                    max_lines=20
                )
                
                predict_btn = gr.Button("ğŸ¯ é¢„æµ‹æ„å›¾", variant="primary", size="lg")
                
                gr.Markdown("### ğŸ“‹ å¿«é€Ÿç¤ºä¾‹")
                gr.Examples(
                    examples=examples,
                    inputs=input_text,
                    label="ç‚¹å‡»ä½¿ç”¨ç¤ºä¾‹"
                )
            
            with gr.Column(scale=1):
                result_text = gr.Markdown(label="é¢„æµ‹ç»“æœ")
                
                with gr.Accordion("ğŸ“Š Top-5 é¢„æµ‹ç»“æœ", open=True):
                    top_k_table = gr.Dataframe(
                        headers=['æ’å', 'æ„å›¾', 'ç½®ä¿¡åº¦', 'ç™¾åˆ†æ¯”'],
                        label="Top-5 é¢„æµ‹"
                    )
                
                with gr.Accordion("ğŸ“ˆ æ‰€æœ‰ç±»åˆ«æ¦‚ç‡åˆ†å¸ƒ", open=False):
                    prob_plot = gr.BarPlot(
                        x="æ„å›¾",
                        y="æ¦‚ç‡",
                        title="æ„å›¾åˆ†ç±»æ¦‚ç‡åˆ†å¸ƒ",
                        x_title="æ„å›¾ç±»åˆ«",
                        y_title="æ¦‚ç‡",
                        height=400,
                        width=600,
                    )
        
        # ç»‘å®šé¢„æµ‹å‡½æ•°
        predict_btn.click(
            fn=predictor.predict_from_turns,
            inputs=input_text,
            outputs=[result_text, top_k_table, prob_plot]
        )
        
        gr.Markdown("""
        ---
        ### ğŸ’¡ æ„å›¾ç±»åˆ«è¯´æ˜
        
        æœ¬ç³»ç»Ÿæ”¯æŒä»¥ä¸‹12ç§æ™ºèƒ½åŠ©æ‰‹å¯¹è¯æ„å›¾åˆ†ç±»ï¼š
        - **é—®é¢˜æ±‚åŠ©/æŠ€æœ¯æ”¯æŒ**: ç”¨æˆ·é‡åˆ°é—®é¢˜å¯»æ±‚å¸®åŠ©ï¼ˆå¦‚WiFiè¿ä¸ä¸Šã€è®¾å¤‡æ•…éšœç­‰ï¼‰
        - **é—²èŠ**: éä»»åŠ¡å¯¼å‘çš„éšæ„èŠå¤©ï¼ˆé—®å€™ã€å…´è¶£è¯é¢˜ç­‰ï¼‰
        - **å¤©æ°”æŸ¥è¯¢**: æŸ¥è¯¢å¤©æ°”ä¿¡æ¯ã€æ¸©åº¦ã€é™é›¨æ¦‚ç‡ç­‰
        - **æ–°é—»èµ„è®¯**: æŸ¥è¯¢å„ç±»æ–°é—»åŠ¨æ€ã€çƒ­ç‚¹äº‹ä»¶
        - **éŸ³ä¹æ’­æ”¾**: æ’­æ”¾éŸ³ä¹ã€åˆ‡æ¢æ­Œæ›²ã€è°ƒæ•´éŸ³é‡ç­‰
        - **æé†’è®¾ç½®**: è®¾ç½®æé†’ã€é—¹é’Ÿã€æŸ¥çœ‹æˆ–ç®¡ç†æé†’
        - **è®¾å¤‡çŠ¶æ€æŸ¥è¯¢**: æŸ¥è¯¢æˆ–è°ƒæ•´è®¾å¤‡çŠ¶æ€ï¼ˆç”µé‡ã€éŸ³é‡ã€å­˜å‚¨ã€ç½‘ç»œç­‰ï¼‰
        - **é€šè¯è§†é¢‘**: å‘èµ·é€šè¯ã€è§†é¢‘ã€è°ƒæ•´é€šè¯è®¾ç½®
        - **è·Œå€’ç›‘æµ‹ç¡®è®¤**: ç”¨æˆ·è·Œå€’æ£€æµ‹è¯†åˆ«åè¿›è¡Œç¡®è®¤
        - **ç©ºé—´æ“ä½œ**: æ§åˆ¶æœºå™¨äººåœ¨ç©ºé—´ä¸­ç§»åŠ¨ã€æ“ä½œç‰©ä½“
        - **è§†è§‰è¯†åˆ«**: è¯†åˆ«ç‰©ä½“ã€æ–‡å­—OCRã€å›¾åƒåˆ†æ
        - **é¤å…æ¨è**: æ¨èé¤å…åœºæ‰€
        """)
    
    return demo


def main():
    parser = argparse.ArgumentParser(description='å¯¹è¯æ„å›¾åˆ†ç±»Web Demo')
    parser.add_argument(
        '--checkpoint',
        type=str,
        default='checkpoints/teacher/best_model.pt',
        help='æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„'
    )
    parser.add_argument(
        '--config',
        type=str,
        default='config/teacher_config.yaml',
        help='é…ç½®æ–‡ä»¶è·¯å¾„'
    )
    parser.add_argument(
        '--port',
        type=int,
        default=7860,
        help='WebæœåŠ¡ç«¯å£'
    )
    parser.add_argument(
        '--share',
        action='store_true',
        help='ç”Ÿæˆå…¬ç½‘åˆ†äº«é“¾æ¥'
    )
    
    args = parser.parse_args()
    
    # åˆ›å»ºé¢„æµ‹å™¨
    print("\næ­£åœ¨åŠ è½½æ¨¡å‹...")
    predictor = WebPredictor(
        checkpoint_path=args.checkpoint,
        config_path=args.config
    )
    
    # åˆ›å»ºå¹¶å¯åŠ¨Demo
    print("\næ­£åœ¨å¯åŠ¨Webç•Œé¢...")
    demo = create_demo(predictor)
    demo.launch(
        server_port=args.port,
        share=args.share,
        server_name="0.0.0.0"
    )


if __name__ == '__main__':
    main()

