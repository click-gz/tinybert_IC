# BERTæ·±åº¦å­¦ä¹ æŒ‡å—ï¼šä»åŸç†åˆ°éƒ¨ç½²

## ç›®å½•

1. [BERTåŸºç¡€åŸç†](#1-bertåŸºç¡€åŸç†)
2. [Transformeræ¶æ„è¯¦è§£](#2-transformeræ¶æ„è¯¦è§£)
3. [BERTé¢„è®­ç»ƒæœºåˆ¶](#3-berté¢„è®­ç»ƒæœºåˆ¶)
4. [BERTå¾®è°ƒæŠ€æœ¯](#4-bertå¾®è°ƒæŠ€æœ¯)
5. [çŸ¥è¯†è’¸é¦åŸç†ä¸å®è·µ](#5-çŸ¥è¯†è’¸é¦åŸç†ä¸å®è·µ)
6. [TinyBERTæ·±åº¦è§£æ](#6-tinybertæ·±åº¦è§£æ)
7. [ONNXæ¨¡å‹éƒ¨ç½²](#7-onnxæ¨¡å‹éƒ¨ç½²)
8. [é¢è¯•é«˜é¢‘é—®é¢˜](#8-é¢è¯•é«˜é¢‘é—®é¢˜)

---

## 1. BERTåŸºç¡€åŸç†

### 1.1 BERTæ¦‚è¿°

**BERT (Bidirectional Encoder Representations from Transformers)** æ˜¯Googleåœ¨2018å¹´æå‡ºçš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œå¼•å‘äº†NLPé¢†åŸŸçš„é©å‘½æ€§å˜åŒ–ã€‚

#### æ ¸å¿ƒåˆ›æ–°ç‚¹
- **åŒå‘ç¼–ç **ï¼šåŒæ—¶è€ƒè™‘ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè€Œéä¼ ç»Ÿçš„å•å‘ï¼ˆä»å·¦åˆ°å³æˆ–ä»å³åˆ°å·¦ï¼‰
- **é¢„è®­ç»ƒ+å¾®è°ƒèŒƒå¼**ï¼šå¤§è§„æ¨¡æ— æ ‡æ³¨æ•°æ®é¢„è®­ç»ƒ + å°è§„æ¨¡æœ‰æ ‡æ³¨æ•°æ®å¾®è°ƒ
- **Transformeræ¶æ„**ï¼šå®Œå…¨åŸºäºæ³¨æ„åŠ›æœºåˆ¶ï¼Œæ‘’å¼ƒRNN/CNNç»“æ„
- **é€šç”¨æ€§å¼º**ï¼šä¸€ä¸ªæ¨¡å‹é€‚é…å¤šç§ä¸‹æ¸¸ä»»åŠ¡

### 1.2 BERTæ¨¡å‹æ¶æ„

#### æ¨¡å‹è§„æ¨¡
```
BERT-Base:
- å±‚æ•°(L): 12
- éšè—å±‚ç»´åº¦(H): 768
- æ³¨æ„åŠ›å¤´æ•°(A): 12
- å‚æ•°é‡: 110M

BERT-Large:
- å±‚æ•°(L): 24
- éšè—å±‚ç»´åº¦(H): 1024
- æ³¨æ„åŠ›å¤´æ•°(A): 16
- å‚æ•°é‡: 340M
```

#### è¾“å…¥è¡¨ç¤º

BERTçš„è¾“å…¥ç”±ä¸‰éƒ¨åˆ†embeddingç›¸åŠ æ„æˆï¼š

```
Input Embedding = Token Embedding + Segment Embedding + Position Embedding
```

**1. Token Embeddingï¼ˆè¯åµŒå…¥ï¼‰**
- ä½¿ç”¨WordPiece tokenization
- è¯æ±‡è¡¨å¤§å°ï¼š30,000
- ç‰¹æ®Šæ ‡è®°ï¼š
  - `[CLS]`ï¼šå¥é¦–æ ‡è®°ï¼Œç”¨äºåˆ†ç±»ä»»åŠ¡
  - `[SEP]`ï¼šå¥å­åˆ†éš”ç¬¦
  - `[MASK]`ï¼šæ©ç æ ‡è®°ï¼Œç”¨äºMLMä»»åŠ¡
  - `[PAD]`ï¼šå¡«å……æ ‡è®°
  - `[UNK]`ï¼šæœªçŸ¥è¯æ ‡è®°

**2. Segment Embeddingï¼ˆæ®µåµŒå…¥ï¼‰**
- ç”¨äºåŒºåˆ†å¥å­Aå’Œå¥å­B
- åªæœ‰ä¸¤ä¸ªå€¼ï¼šEAå’ŒEB
- å•å¥ä»»åŠ¡æ—¶å…¨éƒ¨ä½¿ç”¨EA

**3. Position Embeddingï¼ˆä½ç½®åµŒå…¥ï¼‰**
- å­¦ä¹ å¾—åˆ°çš„ä½ç½®ç¼–ç ï¼ˆä¸TransformeråŸå§‹è®ºæ–‡ä¸åŒï¼‰
- æœ€å¤§åºåˆ—é•¿åº¦ï¼š512
- ä¸ºæ¯ä¸ªä½ç½®å­¦ä¹ ä¸€ä¸ªå›ºå®šçš„embeddingå‘é‡

#### æ¨¡å‹ç»“æ„å±‚æ¬¡

```
è¾“å…¥å±‚ (Input Layer)
    â†“
Token/Segment/Position Embeddings
    â†“
Layer Normalization
    â†“
Transformer Encoder Ã— Lå±‚
    â”œâ”€ Multi-Head Self-Attention
    â”œâ”€ Add & Norm
    â”œâ”€ Feed Forward Network
    â””â”€ Add & Norm
    â†“
è¾“å‡ºå±‚ (Output Layer)
```

### 1.3 BERTä¸å…¶ä»–æ¨¡å‹å¯¹æ¯”

| æ¨¡å‹ | æ–¹å‘æ€§ | æ¶æ„ | ä¼˜åŠ¿ | åŠ£åŠ¿ |
|------|--------|------|------|------|
| **BERT** | åŒå‘ | Encoder-only | ç†è§£èƒ½åŠ›å¼ºï¼Œé€‚åˆåˆ†ç±»/NER | ç”Ÿæˆèƒ½åŠ›å¼± |
| **GPT** | å•å‘ï¼ˆå·¦â†’å³ï¼‰| Decoder-only | ç”Ÿæˆèƒ½åŠ›å¼º | ç†è§£ä¸å¦‚BERT |
| **ELMo** | åŒå‘ï¼ˆç‹¬ç«‹æ‹¼æ¥ï¼‰| LSTM | å­—ç¬¦çº§ï¼ŒOOVå¤„ç†å¥½ | åŒå‘éçœŸæ­£èåˆ |
| **XLNet** | åŒå‘ï¼ˆæ’åˆ—è¯­è¨€æ¨¡å‹ï¼‰| Transformer-XL | é¿å…MASKé¢„æµ‹åå·® | è®­ç»ƒå¤æ‚ |

---

## 2. Transformeræ¶æ„è¯¦è§£

### 2.1 è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆSelf-Attentionï¼‰

#### æ•°å­¦åŸç†

å¯¹äºè¾“å…¥åºåˆ— X = [xâ‚, xâ‚‚, ..., xâ‚™]ï¼Œè‡ªæ³¨æ„åŠ›è®¡ç®—ï¼š

```
1. çº¿æ€§å˜æ¢ï¼š
   Q = XWq  (Query)
   K = XWk  (Key)
   V = XWv  (Value)

2. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼š
   Attention(Q, K, V) = softmax(QK^T / âˆšdk) V
```

**ä¸ºä»€ä¹ˆé™¤ä»¥âˆšdkï¼Ÿ**
- dkæ˜¯Keyçš„ç»´åº¦
- å½“dkå¾ˆå¤§æ—¶ï¼Œç‚¹ç§¯ç»“æœæ–¹å·®ä¼šå¾ˆå¤§
- ä¼šå¯¼è‡´softmaxå‡½æ•°æ¢¯åº¦å¾ˆå°
- é™¤ä»¥âˆšdkè¿›è¡Œç¼©æ”¾ï¼Œç¨³å®šæ¢¯åº¦

#### å¤šå¤´æ³¨æ„åŠ›ï¼ˆMulti-Head Attentionï¼‰

```python
MultiHead(Q, K, V) = Concat(headâ‚, ..., headâ‚•)W^O

å…¶ä¸­ headáµ¢ = Attention(QWáµ¢Q, KWáµ¢K, VWáµ¢V)
```

**ä¸ºä»€ä¹ˆéœ€è¦å¤šå¤´ï¼Ÿ**
- ä¸åŒçš„å¤´å¯ä»¥å…³æ³¨ä¸åŒçš„è¯­ä¹‰å­ç©ºé—´
- headâ‚å¯èƒ½å…³æ³¨è¯­æ³•å…³ç³»
- headâ‚‚å¯èƒ½å…³æ³¨è¯­ä¹‰ç›¸ä¼¼æ€§
- headâ‚ƒå¯èƒ½å…³æ³¨é•¿è·ç¦»ä¾èµ–
- å¢å¼ºæ¨¡å‹è¡¨è¾¾èƒ½åŠ›

### 2.2 å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆFeed Forward Networkï¼‰

```python
FFN(x) = max(0, xWâ‚ + bâ‚)Wâ‚‚ + bâ‚‚
```

- ä¸¤å±‚å…¨è¿æ¥ç½‘ç»œ
- ä¸­é—´å±‚ç»´åº¦é€šå¸¸æ˜¯éšè—å±‚çš„4å€ï¼ˆ768 â†’ 3072 â†’ 768ï¼‰
- æ¿€æ´»å‡½æ•°ï¼šGELUï¼ˆGaussian Error Linear Unitï¼‰

**GELU vs ReLUï¼š**
```
ReLU(x) = max(0, x)
GELU(x) = x * Î¦(x)  # Î¦æ˜¯æ ‡å‡†é«˜æ–¯åˆ†å¸ƒçš„ç´¯ç§¯åˆ†å¸ƒå‡½æ•°
```
GELUæ›´å¹³æ»‘ï¼Œåœ¨è´Ÿå€¼åŒºåŸŸæœ‰å°æ¢¯åº¦ï¼Œè®­ç»ƒæ•ˆæœæ›´å¥½ã€‚

### 2.3 Layer Normalization

```python
LayerNorm(x) = Î³ * (x - Î¼) / âˆš(ÏƒÂ² + Îµ) + Î²
```

- å¯¹æ¯ä¸ªæ ·æœ¬çš„ç‰¹å¾ç»´åº¦è¿›è¡Œå½’ä¸€åŒ–
- ä¸åŒäºBatch Normalizationï¼ˆå¯¹batchç»´åº¦å½’ä¸€åŒ–ï¼‰
- é€‚åˆåºåˆ—æ¨¡å‹ï¼Œä¸å—batch sizeå½±å“

**æ®‹å·®è¿æ¥ï¼ˆResidual Connectionï¼‰ï¼š**
```python
output = LayerNorm(x + SubLayer(x))
```

### 2.4 Transformer Encoder Layerå®Œæ•´è®¡ç®—æµç¨‹

```python
# ä¼ªä»£ç 
def transformer_encoder_layer(x):
    # Multi-Head Self-Attention
    attn_output = multi_head_attention(Q=x, K=x, V=x)
    x = layer_norm(x + dropout(attn_output))  # Add & Norm
    
    # Feed Forward Network
    ffn_output = feed_forward(x)
    x = layer_norm(x + dropout(ffn_output))  # Add & Norm
    
    return x
```

---

## 3. BERTé¢„è®­ç»ƒæœºåˆ¶

### 3.1 æ©ç è¯­è¨€æ¨¡å‹ï¼ˆMasked Language Model, MLMï¼‰

#### åŸºæœ¬åŸç†
éšæœºé®ç›–è¾“å…¥ä¸­15%çš„tokenï¼Œè®©æ¨¡å‹é¢„æµ‹è¢«é®ç›–çš„è¯ã€‚

#### é®ç›–ç­–ç•¥
å¯¹äºé€‰ä¸­è¦é®ç›–çš„15%çš„tokenï¼š
- **80%** çš„æ—¶é—´ï¼šæ›¿æ¢ä¸º `[MASK]`
  - ä¾‹ï¼š`my dog is hairy` â†’ `my dog is [MASK]`
- **10%** çš„æ—¶é—´ï¼šæ›¿æ¢ä¸ºéšæœºè¯
  - ä¾‹ï¼š`my dog is hairy` â†’ `my dog is apple`
- **10%** çš„æ—¶é—´ï¼šä¿æŒä¸å˜
  - ä¾‹ï¼š`my dog is hairy` â†’ `my dog is hairy`

**ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡ï¼Ÿ**
- å¦‚æœ100%æ›¿æ¢ä¸º`[MASK]`ï¼Œä¼šé€ æˆé¢„è®­ç»ƒå’Œå¾®è°ƒçš„ä¸åŒ¹é…ï¼ˆå¾®è°ƒæ—¶æ²¡æœ‰`[MASK]`ï¼‰
- éšæœºæ›¿æ¢ï¼šè®©æ¨¡å‹å­¦ä¼šçº é”™èƒ½åŠ›
- ä¿æŒä¸å˜ï¼šè®©æ¨¡å‹å­¦ä¹ çœŸå®è¯çš„è¡¨ç¤º

#### æŸå¤±å‡½æ•°
```python
L_MLM = -âˆ‘ log P(xáµ¢ | xÌ‚)  # ä»…å¯¹è¢«maskçš„tokenè®¡ç®—
```

### 3.2 ä¸‹ä¸€å¥é¢„æµ‹ï¼ˆNext Sentence Prediction, NSPï¼‰

#### ä»»åŠ¡è®¾è®¡
ç»™å®šå¥å­å¯¹(A, B)ï¼Œé¢„æµ‹Bæ˜¯å¦æ˜¯Açš„ä¸‹ä¸€å¥ã€‚

```
è¾“å…¥ï¼š[CLS] å¥å­A [SEP] å¥å­B [SEP]
è¾“å‡ºï¼šIsNext / NotNext
```

#### è®­ç»ƒæ•°æ®æ„é€ 
- **50%** æ­£æ ·æœ¬ï¼šBç¡®å®æ˜¯Açš„ä¸‹ä¸€å¥
- **50%** è´Ÿæ ·æœ¬ï¼šBæ˜¯è¯­æ–™åº“ä¸­éšæœºé€‰æ‹©çš„å¥å­

#### ä¸ºä»€ä¹ˆéœ€è¦NSPï¼Ÿ
- å­¦ä¹ å¥å­é—´å…³ç³»
- é€‚é…QAã€NLIç­‰éœ€è¦ç†è§£å¥å­å¯¹å…³ç³»çš„ä»»åŠ¡
- å¢å¼ºæ¨¡å‹çš„è¯­ä¹‰ç†è§£èƒ½åŠ›

**æ³¨æ„ï¼š** RoBERTaç­‰åç»­ç ”ç©¶å‘ç°NSPä»»åŠ¡å¯èƒ½ä¸æ˜¯å¿…éœ€çš„ï¼Œç”šè‡³æœ‰è´Ÿé¢å½±å“ã€‚

### 3.3 é¢„è®­ç»ƒæ•°æ®ä¸æµç¨‹

#### è®­ç»ƒæ•°æ®
- **BooksCorpus**ï¼š800Mè¯
- **English Wikipedia**ï¼š2,500Mè¯
- æ€»è®¡ï¼šçº¦3.3Bè¯

#### è®­ç»ƒç»†èŠ‚
```
ä¼˜åŒ–å™¨ï¼šAdam
å­¦ä¹ ç‡ï¼š1e-4
Warm-upæ­¥æ•°ï¼š10,000æ­¥
æ‰¹æ¬¡å¤§å°ï¼š256åºåˆ—
æœ€å¤§åºåˆ—é•¿åº¦ï¼š512
è®­ç»ƒæ­¥æ•°ï¼š1,000,000æ­¥
ç¡¬ä»¶ï¼š16ä¸ªTPUï¼ˆBERT-Baseï¼‰/ 64ä¸ªTPUï¼ˆBERT-Largeï¼‰
è®­ç»ƒæ—¶é—´ï¼š4å¤©ï¼ˆBERT-Baseï¼‰
```

#### è®­ç»ƒæŠ€å·§
1. **Learning Rate Warm-up**ï¼šå‰10,000æ­¥çº¿æ€§å¢åŠ å­¦ä¹ ç‡
2. **Linear Decay**ï¼šä¹‹åçº¿æ€§è¡°å‡è‡³0
3. **Dropout**ï¼šæ‰€æœ‰å±‚ä½¿ç”¨0.1çš„dropout
4. **Gradient Clipping**ï¼šè£å‰ªæ¢¯åº¦é˜²æ­¢çˆ†ç‚¸

---

## 4. BERTå¾®è°ƒæŠ€æœ¯

### 4.1 å¾®è°ƒèŒƒå¼

BERTçš„å¾®è°ƒéµå¾ªä»¥ä¸‹æ¨¡å¼ï¼š
```
é¢„è®­ç»ƒæ¨¡å‹å‚æ•° â†’ æ·»åŠ ä»»åŠ¡ç‰¹å®šå±‚ â†’ ç«¯åˆ°ç«¯å¾®è°ƒ
```

### 4.2 å…¸å‹ä¸‹æ¸¸ä»»åŠ¡

#### 4.2.1 æ–‡æœ¬åˆ†ç±»ï¼ˆSingle Sentence Classificationï¼‰

**æ¶æ„ï¼š**
```
[CLS] æ–‡æœ¬ [SEP]
    â†“
BERTç¼–ç 
    â†“
å–[CLS]ä½ç½®çš„è¾“å‡º
    â†“
å…¨è¿æ¥å±‚ + Softmax
    â†“
ç±»åˆ«æ¦‚ç‡åˆ†å¸ƒ
```

**ä»£ç ç¤ºä¾‹ï¼š**
```python
class BertForSequenceClassification(nn.Module):
    def __init__(self, bert_model, num_labels):
        super().__init__()
        self.bert = bert_model
        self.dropout = nn.Dropout(0.1)
        self.classifier = nn.Linear(768, num_labels)
    
    def forward(self, input_ids, attention_mask, token_type_ids):
        # BERTç¼–ç 
        outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids
        )
        
        # å–[CLS]ä½ç½®çš„è¾“å‡º
        pooled_output = outputs.pooler_output  # [batch_size, 768]
        pooled_output = self.dropout(pooled_output)
        
        # åˆ†ç±»
        logits = self.classifier(pooled_output)  # [batch_size, num_labels]
        return logits
```

#### 4.2.2 å¥å­å¯¹åˆ†ç±»ï¼ˆSentence Pair Classificationï¼‰

**å…¸å‹ä»»åŠ¡ï¼š** è‡ªç„¶è¯­è¨€æ¨ç†ï¼ˆNLIï¼‰ã€è¯­ä¹‰ç›¸ä¼¼åº¦

**è¾“å…¥æ ¼å¼ï¼š**
```
[CLS] å‰æå¥ [SEP] å‡è®¾å¥ [SEP]
```

**ç¤ºä¾‹ï¼ˆNLIï¼‰ï¼š**
```
å‰æï¼šä¸€ä¸ªç”·äººåœ¨å¼¹å‰ä»–
å‡è®¾ï¼šä¸€ä¸ªäººåœ¨æ¼”å¥ä¹å™¨
æ ‡ç­¾ï¼šEntailmentï¼ˆè•´å«ï¼‰
```

#### 4.2.3 å‘½åå®ä½“è¯†åˆ«ï¼ˆNamed Entity Recognition, NERï¼‰

**æ¶æ„ï¼š**
```
[CLS] wâ‚ wâ‚‚ ... wâ‚™ [SEP]
    â†“
BERTç¼–ç 
    â†“
æ¯ä¸ªtokençš„è¾“å‡º
    â†“
å…¨è¿æ¥å±‚ + Softmaxï¼ˆæ¯ä¸ªtokenç‹¬ç«‹åˆ†ç±»ï¼‰
    â†“
BIOæ ‡æ³¨åºåˆ—
```

**ä»£ç ç¤ºä¾‹ï¼š**
```python
class BertForTokenClassification(nn.Module):
    def __init__(self, bert_model, num_labels):
        super().__init__()
        self.bert = bert_model
        self.dropout = nn.Dropout(0.1)
        self.classifier = nn.Linear(768, num_labels)
    
    def forward(self, input_ids, attention_mask, token_type_ids):
        # BERTç¼–ç 
        outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids
        )
        
        # å–æ‰€æœ‰tokençš„è¾“å‡º
        sequence_output = outputs.last_hidden_state  # [batch, seq_len, 768]
        sequence_output = self.dropout(sequence_output)
        
        # æ¯ä¸ªtokenåˆ†ç±»
        logits = self.classifier(sequence_output)  # [batch, seq_len, num_labels]
        return logits
```

#### 4.2.4 é—®ç­”ç³»ç»Ÿï¼ˆQuestion Answeringï¼‰

**ä»»åŠ¡ï¼š** ä»æ®µè½ä¸­æ‰¾å‡ºç­”æ¡ˆçš„èµ·å§‹å’Œç»“æŸä½ç½®

**è¾“å…¥æ ¼å¼ï¼š**
```
[CLS] é—®é¢˜ [SEP] æ®µè½ [SEP]
```

**è¾“å‡ºï¼š**
- èµ·å§‹ä½ç½®æ¦‚ç‡åˆ†å¸ƒ
- ç»“æŸä½ç½®æ¦‚ç‡åˆ†å¸ƒ

**ä»£ç ç¤ºä¾‹ï¼š**
```python
class BertForQuestionAnswering(nn.Module):
    def __init__(self, bert_model):
        super().__init__()
        self.bert = bert_model
        self.qa_outputs = nn.Linear(768, 2)  # èµ·å§‹å’Œç»“æŸä½ç½®
    
    def forward(self, input_ids, attention_mask, token_type_ids):
        # BERTç¼–ç 
        outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids
        )
        
        sequence_output = outputs.last_hidden_state  # [batch, seq_len, 768]
        logits = self.qa_outputs(sequence_output)  # [batch, seq_len, 2]
        
        start_logits, end_logits = logits.split(1, dim=-1)
        start_logits = start_logits.squeeze(-1)  # [batch, seq_len]
        end_logits = end_logits.squeeze(-1)  # [batch, seq_len]
        
        return start_logits, end_logits
```

### 4.3 å¾®è°ƒæœ€ä½³å®è·µ

#### è¶…å‚æ•°é€‰æ‹©

| å‚æ•° | æ¨èå€¼ | è¯´æ˜ |
|------|--------|------|
| **Batch Size** | 16, 32 | å°æ•°æ®é›†ç”¨16ï¼Œå¤§æ•°æ®é›†ç”¨32 |
| **Learning Rate** | 5e-5, 3e-5, 2e-5 | é€šå¸¸éœ€è¦ç½‘æ ¼æœç´¢ |
| **Epochs** | 2-4 | è¿‡å¤šå®¹æ˜“è¿‡æ‹Ÿåˆ |
| **Max Seq Length** | 128, 256, 512 | æ ¹æ®ä»»åŠ¡é€‰æ‹© |
| **Warmup Proportion** | 0.1 | è®­ç»ƒæ­¥æ•°çš„10% |

#### å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥

```python
# çº¿æ€§Warmup + çº¿æ€§è¡°å‡
def get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):
    def lr_lambda(current_step):
        if current_step < num_warmup_steps:
            # Warmupé˜¶æ®µï¼šçº¿æ€§å¢é•¿
            return float(current_step) / float(max(1, num_warmup_steps))
        # è¡°å‡é˜¶æ®µï¼šçº¿æ€§è¡°å‡
        return max(0.0, float(num_training_steps - current_step) / 
                   float(max(1, num_training_steps - num_warmup_steps)))
    
    return LambdaLR(optimizer, lr_lambda)
```

#### é˜²æ­¢è¿‡æ‹ŸåˆæŠ€å·§

1. **Early Stopping**ï¼šéªŒè¯é›†æ€§èƒ½ä¸å†æå‡æ—¶åœæ­¢
2. **Dropout**ï¼šä¿æŒ0.1çš„dropoutç‡
3. **Weight Decay**ï¼šL2æ­£åˆ™åŒ–ï¼Œç³»æ•°0.01
4. **Gradient Clipping**ï¼šè£å‰ªæ¢¯åº¦èŒƒæ•°è‡³1.0
5. **æ•°æ®å¢å¼º**ï¼šåŒä¹‰è¯æ›¿æ¢ã€å›è¯‘ç­‰
6. **å¯¹æŠ—è®­ç»ƒ**ï¼šFGMã€PGDç­‰æ–¹æ³•

#### Layer-wise Learning Rate Decay

ä¸åŒå±‚ä½¿ç”¨ä¸åŒå­¦ä¹ ç‡ï¼š
```python
# åº•å±‚ï¼ˆé è¿‘è¾“å…¥ï¼‰å­¦ä¹ ç‡å°ï¼Œé¡¶å±‚å­¦ä¹ ç‡å¤§
lr_layer_i = base_lr * (decay_rate ** (num_layers - i))
```

åŸç†ï¼šåº•å±‚å­¦ä¹ åˆ°çš„æ˜¯é€šç”¨ç‰¹å¾ï¼Œé¡¶å±‚æ˜¯ä»»åŠ¡ç‰¹å®šç‰¹å¾ã€‚

---

## 5. çŸ¥è¯†è’¸é¦åŸç†ä¸å®è·µ

### 5.1 çŸ¥è¯†è’¸é¦åŸºç¡€

#### æ ¸å¿ƒæ€æƒ³
å°†å¤§å‹"æ•™å¸ˆæ¨¡å‹"çš„çŸ¥è¯†è½¬ç§»åˆ°å°å‹"å­¦ç”Ÿæ¨¡å‹"ï¼Œåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶å‡å°‘æ¨¡å‹å¤§å°å’Œæ¨ç†æ—¶é—´ã€‚

#### ä¸ºä»€ä¹ˆéœ€è¦è’¸é¦ï¼Ÿ
- **éƒ¨ç½²éœ€æ±‚**ï¼šç§»åŠ¨ç«¯ã€åµŒå…¥å¼è®¾å¤‡èµ„æºæœ‰é™
- **æ¨ç†é€Ÿåº¦**ï¼šå®æ—¶åº”ç”¨éœ€è¦ä½å»¶è¿Ÿ
- **æˆæœ¬è€ƒè™‘**ï¼šå‡å°‘è®¡ç®—å’Œå­˜å‚¨æˆæœ¬

### 5.2 ç»å…¸çŸ¥è¯†è’¸é¦ï¼ˆHinton et al.ï¼‰

#### è½¯æ ‡ç­¾ï¼ˆSoft Targetsï¼‰

æ•™å¸ˆæ¨¡å‹çš„è¾“å‡ºåˆ†å¸ƒåŒ…å«"æš—çŸ¥è¯†"ï¼š

```python
# ç¡¬æ ‡ç­¾
y_hard = [0, 0, 1, 0, 0]  # one-hot

# è½¯æ ‡ç­¾ï¼ˆæ•™å¸ˆæ¨¡å‹è¾“å‡ºï¼‰
y_soft = [0.01, 0.05, 0.85, 0.06, 0.03]  # åŒ…å«ç±»é—´å…³ç³»ä¿¡æ¯
```

#### æ¸©åº¦ç¼©æ”¾ï¼ˆTemperature Scalingï¼‰

```python
q_i = exp(z_i / T) / âˆ‘_j exp(z_j / T)

å…¶ä¸­ï¼š
- z_iï¼šlogitsï¼ˆæœªå½’ä¸€åŒ–çš„è¾“å‡ºï¼‰
- Tï¼šæ¸©åº¦å‚æ•°
- T=1ï¼šæ ‡å‡†softmax
- T>1ï¼šè¾“å‡ºåˆ†å¸ƒæ›´å¹³æ»‘ï¼ˆè½¯åŒ–ï¼‰
```

**æ¸©åº¦çš„ä½œç”¨ï¼š**
- Tâ†‘ï¼šåˆ†å¸ƒæ›´å¹³æ»‘ï¼Œæš—çŸ¥è¯†æ›´æ˜æ˜¾
- T=1ï¼šæ­£å¸¸åˆ†å¸ƒ
- è®­ç»ƒæ—¶ï¼šä½¿ç”¨é«˜æ¸©ï¼ˆå¦‚T=3-5ï¼‰
- æ¨ç†æ—¶ï¼šä½¿ç”¨T=1

#### è’¸é¦æŸå¤±å‡½æ•°

```python
L_KD = Î±L_soft + (1-Î±)L_hard

L_soft = KL_Divergence(Student(x,T), Teacher(x,T)) * TÂ²
L_hard = CrossEntropy(Student(x,1), y_true)

å…¶ä¸­ï¼š
- Î±ï¼šè½¯æ ‡ç­¾æŸå¤±æƒé‡ï¼ˆé€šå¸¸0.5-0.9ï¼‰
- TÂ²ï¼šæ¸©åº¦å¹³æ–¹é¡¹ç”¨äºè¡¥å¿æ¢¯åº¦ç¼©æ”¾
```

**ä»£ç å®ç°ï¼š**
```python
import torch.nn.functional as F

def distillation_loss(student_logits, teacher_logits, labels, T=3.0, alpha=0.5):
    # è½¯æ ‡ç­¾æŸå¤±
    soft_loss = F.kl_div(
        F.log_softmax(student_logits / T, dim=-1),
        F.softmax(teacher_logits / T, dim=-1),
        reduction='batchmean'
    ) * (T * T)
    
    # ç¡¬æ ‡ç­¾æŸå¤±
    hard_loss = F.cross_entropy(student_logits, labels)
    
    # ç»„åˆæŸå¤±
    return alpha * soft_loss + (1 - alpha) * hard_loss
```

### 5.3 é«˜çº§è’¸é¦æŠ€æœ¯

#### 5.3.1 ç‰¹å¾è’¸é¦ï¼ˆFeature Distillationï¼‰

ä¸ä»…è’¸é¦è¾“å‡ºï¼Œè¿˜è’¸é¦ä¸­é—´å±‚ç‰¹å¾ï¼š

```python
L_feature = MSE(Transform(H_student), H_teacher)

å…¶ä¸­ï¼š
- Hï¼šä¸­é—´å±‚éšè—çŠ¶æ€
- Transformï¼šç»´åº¦å¯¹é½å˜æ¢ï¼ˆå¦‚æœå­¦ç”Ÿå’Œæ•™å¸ˆç»´åº¦ä¸åŒï¼‰
```

#### 5.3.2 æ³¨æ„åŠ›è’¸é¦ï¼ˆAttention Distillationï¼‰

è’¸é¦è‡ªæ³¨æ„åŠ›æƒé‡çŸ©é˜µï¼š

```python
L_attn = MSE(A_student, A_teacher)

å…¶ä¸­ A âˆˆ R^(seq_len Ã— seq_len) æ˜¯æ³¨æ„åŠ›æƒé‡çŸ©é˜µ
```

#### 5.3.3 å…³ç³»è’¸é¦ï¼ˆRelation Distillationï¼‰

è’¸é¦æ ·æœ¬é—´çš„å…³ç³»ï¼š

```python
# æ ·æœ¬é—´ä½™å¼¦ç›¸ä¼¼åº¦
R_teacher[i,j] = cos_sim(h_teacher^i, h_teacher^j)
R_student[i,j] = cos_sim(h_student^i, h_student^j)

L_relation = MSE(R_student, R_teacher)
```

### 5.4 BERTè’¸é¦æ–¹æ³•å¯¹æ¯”

| æ–¹æ³• | è’¸é¦ç›®æ ‡ | å­¦ç”Ÿç»“æ„ | æ€§èƒ½ä¿æŒç‡ | å‹ç¼©æ¯” |
|------|----------|----------|-----------|--------|
| **DistilBERT** | è¾“å‡º+ä¸­é—´å±‚ | 6å±‚Transformer | ~97% | 2Ã— |
| **TinyBERT** | è¾“å‡º+ä¸­é—´å±‚+æ³¨æ„åŠ›+åµŒå…¥ | 4å±‚Transformer | ~96% | 7.5Ã— |
| **MobileBERT** | è¾“å‡º+ä¸­é—´å±‚ | çª„è€Œæ·±ï¼ˆ24å±‚ï¼‰ | ~99% | 4Ã— |
| **ALBERT** | å‚æ•°å…±äº« | 12å±‚ï¼ˆå…±äº«å‚æ•°ï¼‰ | ~98% | 18Ã— |

---

## 6. TinyBERTæ·±åº¦è§£æ

### 6.1 TinyBERTåˆ›æ–°ç‚¹

TinyBERTæ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„è’¸é¦æ¡†æ¶ï¼Œé€šè¿‡å…¨é¢çš„çŸ¥è¯†è’¸é¦å®ç°æè‡´å‹ç¼©ã€‚

#### æ ¸å¿ƒåˆ›æ–°
1. **ä¸¤é˜¶æ®µè’¸é¦**ï¼šé€šç”¨è’¸é¦ + ä»»åŠ¡ç‰¹å®šè’¸é¦
2. **Transformerå±‚çº§è’¸é¦**ï¼šåµŒå…¥å±‚ã€æ³¨æ„åŠ›å±‚ã€éšè—å±‚ã€é¢„æµ‹å±‚å…¨æ–¹ä½è’¸é¦
3. **å±‚æ˜ å°„ç­–ç•¥**ï¼šæ•™å¸ˆå±‚åˆ°å­¦ç”Ÿå±‚çš„å‡åŒ€æ˜ å°„

### 6.2 TinyBERTè’¸é¦ç›®æ ‡

#### å®Œæ•´æŸå¤±å‡½æ•°

```python
L_total = L_embd + L_hidn + L_attn + L_pred
```

#### 6.2.1 åµŒå…¥å±‚è’¸é¦

```python
L_embd = MSE(E_s * W_e, E_t)

å…¶ä¸­ï¼š
- E_s âˆˆ R^(seq_len Ã— d_s)ï¼šå­¦ç”ŸåµŒå…¥
- E_t âˆˆ R^(seq_len Ã— d_t)ï¼šæ•™å¸ˆåµŒå…¥  
- W_e âˆˆ R^(d_s Ã— d_t)ï¼šå¯å­¦ä¹ çš„çº¿æ€§å˜æ¢çŸ©é˜µ
```

#### 6.2.2 éšè—å±‚è’¸é¦

```python
L_hidn = (1/K) âˆ‘_{k=1}^K MSE(H_s^(g(k)) * W_h, H_t^k)

å…¶ä¸­ï¼š
- Kï¼šæ•™å¸ˆå±‚æ•°ï¼ˆå¦‚12ï¼‰
- Mï¼šå­¦ç”Ÿå±‚æ•°ï¼ˆå¦‚4ï¼‰
- g(k)ï¼šå±‚æ˜ å°„å‡½æ•°ï¼Œg(k) = âŒˆk*M/KâŒ‰
- W_hï¼šç»´åº¦å˜æ¢çŸ©é˜µ
```

**å±‚æ˜ å°„ç¤ºä¾‹ï¼ˆ12å±‚â†’4å±‚ï¼‰ï¼š**
```
æ•™å¸ˆå±‚ï¼š1  2  3  4  5  6  7  8  9  10 11 12
         â†“     â†“     â†“        â†“        â†“
å­¦ç”Ÿå±‚ï¼š0     1     2        3        4
```

#### 6.2.3 æ³¨æ„åŠ›è’¸é¦

```python
L_attn = (1/K) âˆ‘_{k=1}^K (1/h) âˆ‘_{i=1}^h MSE(A_s^(g(k),i), A_t^(k,i))

å…¶ä¸­ï¼š
- hï¼šæ³¨æ„åŠ›å¤´æ•°
- A^(k,i) âˆˆ R^(seq_len Ã— seq_len)ï¼šç¬¬kå±‚ç¬¬iä¸ªå¤´çš„æ³¨æ„åŠ›çŸ©é˜µ
```

**ä¸ºä»€ä¹ˆè’¸é¦æ³¨æ„åŠ›ï¼Ÿ**
- æ³¨æ„åŠ›çŸ©é˜µæ•è·äº†tokené—´çš„ä¾èµ–å…³ç³»
- åŒ…å«äº†å¥æ³•å’Œè¯­ä¹‰ä¿¡æ¯
- æ˜¯Transformerçš„æ ¸å¿ƒæœºåˆ¶

#### 6.2.4 é¢„æµ‹å±‚è’¸é¦

```python
# å¯¹äºåˆ†ç±»ä»»åŠ¡
L_pred = -softmax(z_t/T) * log_softmax(z_s/T)

# å¯¹äºMLMä»»åŠ¡
L_pred = CE(logits_s, logits_t)  # ä½¿ç”¨è½¯æ ‡ç­¾
```

### 6.3 TinyBERTä¸¤é˜¶æ®µè®­ç»ƒ

#### é˜¶æ®µ1ï¼šé€šç”¨è’¸é¦ï¼ˆGeneral Distillationï¼‰

**ç›®æ ‡ï¼š** ä»é¢„è®­ç»ƒçš„BERT_baseè’¸é¦åˆ°TinyBERT

**æ•°æ®ï¼š** å¤§è§„æ¨¡æ— æ ‡æ³¨æ–‡æœ¬ï¼ˆä¸BERTé¢„è®­ç»ƒç›¸åŒï¼‰

**ä»»åŠ¡ï¼š** ä»…MLMï¼ˆä¸ä½¿ç”¨NSPï¼‰

**æµç¨‹ï¼š**
```python
# ä¼ªä»£ç 
for epoch in epochs:
    for batch in data_loader:
        # 1. éšæœºmask
        masked_input = apply_mask(batch)
        
        # 2. æ•™å¸ˆå‰å‘ä¼ æ’­
        with torch.no_grad():
            teacher_outputs = teacher_model(masked_input)
        
        # 3. å­¦ç”Ÿå‰å‘ä¼ æ’­
        student_outputs = student_model(masked_input)
        
        # 4. è®¡ç®—è’¸é¦æŸå¤±
        loss = compute_distillation_loss(
            student_outputs, 
            teacher_outputs,
            layers=['embd', 'hidn', 'attn', 'pred']
        )
        
        # 5. åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()
```

#### é˜¶æ®µ2ï¼šä»»åŠ¡ç‰¹å®šè’¸é¦ï¼ˆTask-specific Distillationï¼‰

**ç›®æ ‡ï¼š** é’ˆå¯¹ä¸‹æ¸¸ä»»åŠ¡è¿›ä¸€æ­¥è’¸é¦

**æµç¨‹ï¼š**

**Step 1: æ•°æ®å¢å¼º**
```python
# å¯¹è®­ç»ƒæ•°æ®è¿›è¡Œå¢å¼º
def augment_data(text):
    # æ–¹æ³•1ï¼šå•è¯çº§åˆ«å¢å¼º
    - åŒä¹‰è¯æ›¿æ¢ï¼ˆGloVe/Word2Vecæ‰¾ç›¸ä¼¼è¯ï¼‰
    - éšæœºæ’å…¥
    - éšæœºåˆ é™¤
    - éšæœºäº¤æ¢
    
    # æ–¹æ³•2ï¼šå¥å­çº§åˆ«å¢å¼º
    - å›è¯‘ï¼ˆè‹±â†’å¾·â†’è‹±ï¼‰
    - ä¸Šä¸‹æ–‡è¯æ›¿æ¢ï¼ˆBERTé¢„æµ‹maskä½ç½®ï¼‰
    
    return augmented_texts
```

**Step 2: å¾®è°ƒæ•™å¸ˆæ¨¡å‹**
```python
# åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šå¾®è°ƒBERTï¼ˆæ•™å¸ˆï¼‰
teacher_finetuned = finetune_teacher(teacher_bert, task_data)
```

**Step 3: è’¸é¦å­¦ç”Ÿæ¨¡å‹**
```python
# ä½¿ç”¨å¾®è°ƒåçš„æ•™å¸ˆè’¸é¦å­¦ç”Ÿ
for batch in augmented_task_data:
    teacher_outputs = teacher_finetuned(batch)
    student_outputs = student_model(batch)
    
    # åŒ…å«æ‰€æœ‰è’¸é¦æŸå¤±
    loss = L_embd + L_hidn + L_attn + L_pred
    
    loss.backward()
    optimizer.step()
```

### 6.4 TinyBERTå®ç°ç»†èŠ‚

#### æ¨¡å‹é…ç½®

```python
# TinyBERT_4Lï¼ˆ4å±‚ç‰ˆæœ¬ï¼‰
config = {
    'num_hidden_layers': 4,
    'hidden_size': 312,
    'intermediate_size': 1200,
    'num_attention_heads': 12,
    'attention_head_size': 26  # 312 / 12
}

# TinyBERT_6Lï¼ˆ6å±‚ç‰ˆæœ¬ï¼‰
config = {
    'num_hidden_layers': 6,
    'hidden_size': 768,
    'intermediate_size': 3072,
    'num_attention_heads': 12
}
```

#### è®­ç»ƒè¶…å‚æ•°

```python
# é€šç”¨è’¸é¦é˜¶æ®µ
general_distill_config = {
    'learning_rate': 5e-5,
    'batch_size': 256,
    'max_seq_length': 128,
    'num_epochs': 3,
    'warmup_proportion': 0.1,
    'temperature': 1.0,  # æ³¨æ„åŠ›å’Œéšè—å±‚ä¸ä½¿ç”¨æ¸©åº¦
}

# ä»»åŠ¡è’¸é¦é˜¶æ®µ
task_distill_config = {
    'learning_rate': 3e-5,
    'batch_size': 32,
    'num_epochs': 3,
    'augment_ratio': 20,  # æ•°æ®å¢å¼ºå€æ•°
    'temperature': 1.0,
    'alpha': 0.5,  # è½¯ç¡¬æ ‡ç­¾å¹³è¡¡
}
```

### 6.5 TinyBERTæ€§èƒ½åˆ†æ

#### åœ¨GLUEåŸºå‡†ä¸Šçš„è¡¨ç°

| æ¨¡å‹ | å‚æ•°é‡ | æ¨ç†é€Ÿåº¦ | MNLI | QQP | QNLI | SST-2 | å¹³å‡ |
|------|--------|----------|------|-----|------|-------|------|
| BERT-base | 109M | 1Ã— | 84.5 | 89.6 | 91.7 | 92.7 | 89.6 |
| DistilBERT | 66M | 1.7Ã— | 82.2 | 88.5 | 89.2 | 91.3 | 87.8 |
| **TinyBERTâ‚„** | **14.5M** | **9.4Ã—** | **82.8** | **87.7** | **90.4** | **92.6** | **88.4** |
| TinyBERTâ‚† | 67M | 5.3Ã— | 84.6 | 89.1 | 91.6 | 93.1 | 89.6 |

**å…³é”®å‘ç°ï¼š**
- TinyBERTâ‚„ä»…ç”¨14.5Må‚æ•°è¾¾åˆ°BERT-base 96.8%çš„æ€§èƒ½
- æ¨ç†é€Ÿåº¦æå‡9.4å€
- ä»»åŠ¡ç‰¹å®šè’¸é¦å¸¦æ¥æ˜¾è‘—æå‡ï¼ˆ~2-3%ï¼‰

---

## 7. ONNXæ¨¡å‹éƒ¨ç½²

### 7.1 ONNXç®€ä»‹

**ONNX (Open Neural Network Exchange)** æ˜¯ä¸€ä¸ªå¼€æ”¾çš„æ·±åº¦å­¦ä¹ æ¨¡å‹è¡¨ç¤ºæ ‡å‡†ã€‚

#### æ ¸å¿ƒä¼˜åŠ¿
- **è·¨æ¡†æ¶**ï¼šPyTorchã€TensorFlowã€Kerasç­‰äº’é€š
- **è·¨å¹³å°**ï¼šæ”¯æŒå¤šç§ç¡¬ä»¶ï¼ˆCPUã€GPUã€NPUã€ç§»åŠ¨ç«¯ï¼‰
- **é«˜æ€§èƒ½**ï¼šONNX Runtimeä¼˜åŒ–æ¨ç†é€Ÿåº¦
- **æ˜“éƒ¨ç½²**ï¼šç»Ÿä¸€æ ¼å¼ç®€åŒ–ç”Ÿäº§éƒ¨ç½²

### 7.2 BERTæ¨¡å‹å¯¼å‡ºä¸ºONNX

#### 7.2.1 PyTorchæ¨¡å‹å¯¼å‡º

```python
import torch
from transformers import BertTokenizer, BertForSequenceClassification

# 1. åŠ è½½æ¨¡å‹
model = BertForSequenceClassification.from_pretrained('bert-base-uncased')
model.eval()

# 2. å‡†å¤‡è™šæ‹Ÿè¾“å…¥ï¼ˆç”¨äºè¿½è¸ªè®¡ç®—å›¾ï¼‰
batch_size = 1
seq_length = 128
dummy_input = {
    'input_ids': torch.randint(0, 1000, (batch_size, seq_length)),
    'attention_mask': torch.ones(batch_size, seq_length, dtype=torch.long),
    'token_type_ids': torch.zeros(batch_size, seq_length, dtype=torch.long)
}

# 3. å¯¼å‡ºONNX
torch.onnx.export(
    model,
    (dummy_input['input_ids'], 
     dummy_input['attention_mask'], 
     dummy_input['token_type_ids']),
    'bert_model.onnx',
    export_params=True,
    opset_version=14,  # ONNXç®—å­é›†ç‰ˆæœ¬
    do_constant_folding=True,  # å¸¸é‡æŠ˜å ä¼˜åŒ–
    input_names=['input_ids', 'attention_mask', 'token_type_ids'],
    output_names=['logits'],
    dynamic_axes={
        'input_ids': {0: 'batch_size', 1: 'sequence'},
        'attention_mask': {0: 'batch_size', 1: 'sequence'},
        'token_type_ids': {0: 'batch_size', 1: 'sequence'},
        'logits': {0: 'batch_size'}
    }
)
```

**å‚æ•°è¯´æ˜ï¼š**
- `opset_version`ï¼šONNXç®—å­é›†ç‰ˆæœ¬ï¼Œå»ºè®®â‰¥11
- `do_constant_folding`ï¼šç¼–è¯‘æ—¶è®¡ç®—å¸¸é‡ï¼Œå‡å°‘è¿è¡Œæ—¶å¼€é”€
- `dynamic_axes`ï¼šæ”¯æŒåŠ¨æ€batch sizeå’Œåºåˆ—é•¿åº¦

#### 7.2.2 éªŒè¯å¯¼å‡ºçš„æ¨¡å‹

```python
import onnx

# åŠ è½½å¹¶æ£€æŸ¥æ¨¡å‹
onnx_model = onnx.load('bert_model.onnx')
onnx.checker.check_model(onnx_model)

# æ‰“å°æ¨¡å‹ä¿¡æ¯
print(f"ONNXæ¨¡å‹è¾“å…¥: {[input.name for input in onnx_model.graph.input]}")
print(f"ONNXæ¨¡å‹è¾“å‡º: {[output.name for output in onnx_model.graph.output]}")
```

### 7.3 ONNX Runtimeæ¨ç†

#### 7.3.1 åŸºç¡€æ¨ç†

```python
import onnxruntime as ort
import numpy as np

# 1. åˆ›å»ºæ¨ç†ä¼šè¯
session = ort.InferenceSession(
    'bert_model.onnx',
    providers=['CUDAExecutionProvider', 'CPUExecutionProvider']  # GPUä¼˜å…ˆ
)

# 2. å‡†å¤‡è¾“å…¥æ•°æ®
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
text = "ONNX Runtime is awesome!"
encoded = tokenizer(
    text,
    padding='max_length',
    max_length=128,
    truncation=True,
    return_tensors='np'
)

# 3. æ‰§è¡Œæ¨ç†
outputs = session.run(
    None,  # è¾“å‡ºåç§°ï¼ˆNoneè¡¨ç¤ºæ‰€æœ‰è¾“å‡ºï¼‰
    {
        'input_ids': encoded['input_ids'].astype(np.int64),
        'attention_mask': encoded['attention_mask'].astype(np.int64),
        'token_type_ids': encoded['token_type_ids'].astype(np.int64)
    }
)

logits = outputs[0]
predicted_class = np.argmax(logits, axis=-1)
```

#### 7.3.2 æ‰¹é‡æ¨ç†ä¼˜åŒ–

```python
class ONNXBertInference:
    def __init__(self, model_path, max_seq_length=128):
        self.session = ort.InferenceSession(model_path)
        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
        self.max_seq_length = max_seq_length
    
    def predict(self, texts, batch_size=32):
        results = []
        
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i+batch_size]
            
            # æ‰¹é‡ç¼–ç 
            encoded = self.tokenizer(
                batch_texts,
                padding='max_length',
                max_length=self.max_seq_length,
                truncation=True,
                return_tensors='np'
            )
            
            # æ‰¹é‡æ¨ç†
            outputs = self.session.run(
                None,
                {
                    'input_ids': encoded['input_ids'].astype(np.int64),
                    'attention_mask': encoded['attention_mask'].astype(np.int64),
                    'token_type_ids': encoded['token_type_ids'].astype(np.int64)
                }
            )
            
            results.extend(outputs[0])
        
        return np.array(results)

# ä½¿ç”¨
inference = ONNXBertInference('bert_model.onnx')
texts = ["sentence 1", "sentence 2", ...]
predictions = inference.predict(texts, batch_size=32)
```

### 7.4 ONNXæ¨¡å‹ä¼˜åŒ–

#### 7.4.1 å›¾ä¼˜åŒ–

```python
from onnxruntime.transformers import optimizer

# ä½¿ç”¨Transformerä¸“ç”¨ä¼˜åŒ–å™¨
optimized_model = optimizer.optimize_model(
    'bert_model.onnx',
    model_type='bert',
    num_heads=12,
    hidden_size=768,
    optimization_options={
        'enable_gelu_approximation': True,  # GELUè¿‘ä¼¼åŠ é€Ÿ
        'enable_skip_layer_norm': True,      # LayerNormèåˆ
        'enable_embed_layer_norm': True,     # åµŒå…¥å±‚èåˆ
        'enable_bias_gelu': True,            # Bias+GELUèåˆ
    }
)

optimized_model.save_model_to_file('bert_optimized.onnx')
```

**ä¼˜åŒ–æŠ€æœ¯ï¼š**
1. **ç®—å­èåˆ**ï¼šåˆå¹¶è¿ç»­æ“ä½œï¼ˆå¦‚Add+LayerNormï¼‰
2. **å¸¸é‡æŠ˜å **ï¼šé¢„è®¡ç®—å¸¸é‡è¡¨è¾¾å¼
3. **å†—ä½™èŠ‚ç‚¹æ¶ˆé™¤**ï¼šåˆ é™¤æ— ç”¨è®¡ç®—
4. **GELUè¿‘ä¼¼**ï¼šç”¨Tanhè¿‘ä¼¼GELUï¼ŒåŠ é€Ÿè®¡ç®—

#### 7.4.2 é‡åŒ–åŠ é€Ÿ

**åŠ¨æ€é‡åŒ–ï¼ˆDynamic Quantizationï¼‰**
```python
from onnxruntime.quantization import quantize_dynamic, QuantType

quantize_dynamic(
    'bert_model.onnx',
    'bert_quantized.onnx',
    weight_type=QuantType.QUInt8  # æƒé‡INT8é‡åŒ–
)
```

**é™æ€é‡åŒ–ï¼ˆStatic Quantizationï¼‰**
```python
from onnxruntime.quantization import quantize_static, CalibrationDataReader

# å‡†å¤‡æ ¡å‡†æ•°æ®
class BertCalibrationDataReader(CalibrationDataReader):
    def __init__(self, calibration_data):
        self.calibration_data = calibration_data
        self.iter = iter(calibration_data)
    
    def get_next(self):
        try:
            return next(self.iter)
        except StopIteration:
            return None

# æ‰§è¡Œé™æ€é‡åŒ–
quantize_static(
    'bert_model.onnx',
    'bert_static_quantized.onnx',
    calibration_data_reader=BertCalibrationDataReader(calibration_data)
)
```

**é‡åŒ–æ•ˆæœå¯¹æ¯”ï¼š**

| æ–¹æ³• | æ¨¡å‹å¤§å° | æ¨ç†é€Ÿåº¦ | ç²¾åº¦æŸå¤± |
|------|----------|----------|----------|
| FP32åŸå§‹ | 440MB | 1Ã— | 0% |
| åŠ¨æ€é‡åŒ– | 110MB | 2-3Ã— | <1% |
| é™æ€é‡åŒ– | 110MB | 3-4Ã— | <2% |

### 7.5 ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²æ¶æ„

#### 7.5.1 Flask APIéƒ¨ç½²

```python
from flask import Flask, request, jsonify
import onnxruntime as ort
import numpy as np

app = Flask(__name__)

# å…¨å±€åŠ è½½æ¨¡å‹ï¼ˆé¿å…é‡å¤åŠ è½½ï¼‰
class ModelService:
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance.session = ort.InferenceSession(
                'bert_optimized.onnx',
                providers=['CUDAExecutionProvider', 'CPUExecutionProvider']
            )
            cls._instance.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
        return cls._instance
    
    def predict(self, text):
        encoded = self.tokenizer(
            text,
            padding='max_length',
            max_length=128,
            truncation=True,
            return_tensors='np'
        )
        
        outputs = self.session.run(
            None,
            {
                'input_ids': encoded['input_ids'].astype(np.int64),
                'attention_mask': encoded['attention_mask'].astype(np.int64),
                'token_type_ids': encoded['token_type_ids'].astype(np.int64)
            }
        )
        
        return outputs[0]

model_service = ModelService()

@app.route('/predict', methods=['POST'])
def predict():
    data = request.json
    text = data.get('text', '')
    
    if not text:
        return jsonify({'error': 'No text provided'}), 400
    
    try:
        logits = model_service.predict(text)
        prediction = int(np.argmax(logits))
        confidence = float(np.max(softmax(logits)))
        
        return jsonify({
            'prediction': prediction,
            'confidence': confidence
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500

def softmax(x):
    exp_x = np.exp(x - np.max(x))
    return exp_x / exp_x.sum()

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
```

#### 7.5.2 Dockerå®¹å™¨åŒ–

```dockerfile
# Dockerfile
FROM python:3.8-slim

WORKDIR /app

# å®‰è£…ä¾èµ–
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶æ¨¡å‹å’Œä»£ç 
COPY bert_optimized.onnx .
COPY app.py .

# æš´éœ²ç«¯å£
EXPOSE 5000

# å¯åŠ¨æœåŠ¡
CMD ["gunicorn", "-w", "4", "-b", "0.0.0.0:5000", "app:app"]
```

```txt
# requirements.txt
flask==2.0.1
gunicorn==20.1.0
onnxruntime-gpu==1.12.0  # æˆ– onnxruntime for CPU
transformers==4.20.0
numpy==1.21.0
```

**æ„å»ºå’Œè¿è¡Œï¼š**
```bash
# æ„å»ºé•œåƒ
docker build -t bert-onnx-service .

# è¿è¡Œå®¹å™¨
docker run -d -p 5000:5000 --gpus all bert-onnx-service
```

#### 7.5.3 Kuberneteséƒ¨ç½²

```yaml
# deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: bert-inference
spec:
  replicas: 3
  selector:
    matchLabels:
      app: bert-inference
  template:
    metadata:
      labels:
        app: bert-inference
    spec:
      containers:
      - name: bert-service
        image: bert-onnx-service:latest
        ports:
        - containerPort: 5000
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
            nvidia.com/gpu: 1
          limits:
            memory: "4Gi"
            cpu: "2000m"
            nvidia.com/gpu: 1
        livenessProbe:
          httpGet:
            path: /health
            port: 5000
          initialDelaySeconds: 30
          periodSeconds: 10
---
apiVersion: v1
kind: Service
metadata:
  name: bert-inference-service
spec:
  type: LoadBalancer
  selector:
    app: bert-inference
  ports:
  - protocol: TCP
    port: 80
    targetPort: 5000
```

### 7.6 æ€§èƒ½ä¼˜åŒ–æŠ€å·§

#### 7.6.1 ä¼šè¯é…ç½®ä¼˜åŒ–

```python
import onnxruntime as ort

# é«˜çº§ä¼šè¯é…ç½®
sess_options = ort.SessionOptions()

# å¹¶è¡Œä¼˜åŒ–
sess_options.intra_op_num_threads = 8  # ç®—å­å†…å¹¶è¡Œçº¿ç¨‹æ•°
sess_options.inter_op_num_threads = 2  # ç®—å­é—´å¹¶è¡Œçº¿ç¨‹æ•°

# å›¾ä¼˜åŒ–çº§åˆ«
sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL

# å†…å­˜ä¼˜åŒ–
sess_options.enable_mem_pattern = True
sess_options.enable_cpu_mem_arena = True

# åˆ›å»ºä¼šè¯
session = ort.InferenceSession(
    'bert_model.onnx',
    sess_options=sess_options,
    providers=['CUDAExecutionProvider']
)
```

#### 7.6.2 IOç»‘å®šä¼˜åŒ–

```python
# ä½¿ç”¨IOBindingå‡å°‘æ•°æ®æ‹·è´
io_binding = session.io_binding()

# ç»‘å®šè¾“å…¥åˆ°GPU
for name, arr in inputs.items():
    io_binding.bind_cpu_input(name, arr)

# ç»‘å®šè¾“å‡ºåˆ°GPU
io_binding.bind_output('logits')

# æ‰§è¡Œæ¨ç†
session.run_with_iobinding(io_binding)

# è·å–ç»“æœ
outputs = io_binding.copy_outputs_to_cpu()
```

---

## 8. é¢è¯•é«˜é¢‘é—®é¢˜

### 8.1 BERTåŸç†ç±»

**Q1: BERTçš„åŒå‘æ˜¯å¦‚ä½•å®ç°çš„ï¼Ÿ**

A: BERTä½¿ç”¨Transformerçš„Encoderç»“æ„ï¼Œé€šè¿‡Self-Attentionæœºåˆ¶åŒæ—¶çœ‹åˆ°å·¦å³ä¸Šä¸‹æ–‡ï¼š
- ä¼ ç»ŸRNN/LSTMåªèƒ½ä»å·¦åˆ°å³æˆ–ä»å³åˆ°å·¦å•å‘ç¼–ç 
- ELMoè™½ç„¶æ˜¯åŒå‘ï¼Œä½†æ˜¯ä¸¤ä¸ªç‹¬ç«‹çš„å•å‘LSTMæ‹¼æ¥ï¼Œæ²¡æœ‰çœŸæ­£èåˆ
- BERTçš„æ¯ä¸ªtokené€šè¿‡Self-Attentionå¯ä»¥ç›´æ¥å…³æ³¨åˆ°åºåˆ—ä¸­ä»»æ„ä½ç½®çš„token
- MLMé¢„è®­ç»ƒä»»åŠ¡å¼ºåˆ¶æ¨¡å‹å­¦ä¹ åŒå‘è¡¨ç¤º

**Q2: ä¸ºä»€ä¹ˆBERTä¸èƒ½ç”¨äºæ–‡æœ¬ç”Ÿæˆï¼Ÿ**

A: 
- BERTæ˜¯Encoder-onlyæ¶æ„ï¼Œè®¾è®¡ç›®æ ‡æ˜¯ç†è§£è€Œéç”Ÿæˆ
- è®­ç»ƒæ—¶å¯ä»¥çœ‹åˆ°å®Œæ•´åºåˆ—ï¼ˆåŒ…æ‹¬æœªæ¥ä¿¡æ¯ï¼‰ï¼Œæ²¡æœ‰å­¦ä¹ å› æœä¾èµ–
- MLMæ˜¯éšæœºé¢„æµ‹ï¼Œä¸æ˜¯é¡ºåºç”Ÿæˆ
- GPTç­‰Decoderæ¶æ„ä½¿ç”¨Causal Maskingï¼Œåªèƒ½çœ‹åˆ°å‰æ–‡ï¼Œé€‚åˆç”Ÿæˆ

**Q3: [MASK]æ ‡è®°åœ¨é¢„è®­ç»ƒå’Œå¾®è°ƒæ—¶çš„ä¸ä¸€è‡´å¦‚ä½•è§£å†³ï¼Ÿ**

A: BERTé‡‡ç”¨çš„ç­–ç•¥ï¼š
- 80%æ›¿æ¢ä¸º[MASK]ï¼šå­¦ä¹ é¢„æµ‹è¢«é®ç›–çš„è¯
- 10%æ›¿æ¢ä¸ºéšæœºè¯ï¼šå­¦ä¹ çº é”™èƒ½åŠ›
- 10%ä¿æŒä¸å˜ï¼šå­¦ä¹ ä¿ç•™åŸè¯çš„è¡¨ç¤º
- è¿™æ ·å¾®è°ƒæ—¶å³ä½¿æ²¡æœ‰[MASK]ï¼Œæ¨¡å‹ä¹Ÿèƒ½æ­£å¸¸å·¥ä½œ

**Q4: BERTçš„Position Embeddingä¸ºä»€ä¹ˆæ˜¯å­¦ä¹ çš„è€Œä¸æ˜¯å›ºå®šçš„ï¼Ÿ**

A:
- TransformeråŸè®ºæ–‡ä½¿ç”¨æ­£å¼¦/ä½™å¼¦å›ºå®šç¼–ç 
- BERTé€‰æ‹©å¯å­¦ä¹ çš„ä½ç½®ç¼–ç ï¼Œå› ä¸ºï¼š
  - é¢„è®­ç»ƒæ•°æ®è¶³å¤Ÿå¤§ï¼Œå¯ä»¥å­¦åˆ°æ›´å¥½çš„ä½ç½®è¡¨ç¤º
  - å­¦ä¹ å¾—åˆ°çš„ç¼–ç å¯ä»¥é€‚åº”ä»»åŠ¡ç‰¹æ€§
  - æœ€å¤§é•¿åº¦512å›ºå®šï¼Œä¸éœ€è¦å¤–æ¨åˆ°æ›´é•¿åºåˆ—
  - å®éªŒè¯æ˜å­¦ä¹ çš„ç¼–ç æ•ˆæœæ›´å¥½

### 8.2 å¾®è°ƒç±»

**Q5: ä¸ºä»€ä¹ˆBERTå¾®è°ƒè¦ç”¨å°å­¦ä¹ ç‡ï¼Ÿ**

A:
- é¢„è®­ç»ƒå·²ç»å­¦åˆ°é€šç”¨è¯­è¨€è¡¨ç¤ºï¼Œä¸åº”è¯¥è¢«å¤§å¹…æ”¹å˜
- å¤§å­¦ä¹ ç‡ä¼šç ´åé¢„è®­ç»ƒçš„çŸ¥è¯†ï¼Œå¯¼è‡´ç¾éš¾æ€§é—å¿˜
- å…¸å‹å€¼2e-5, 3e-5, 5e-5è¿œå°äºä»å¤´è®­ç»ƒçš„1e-3, 1e-4
- åº•å±‚å­¦ä¹ é€šç”¨ç‰¹å¾ï¼Œåº”è¯¥å˜åŒ–æ›´å°ï¼›é¡¶å±‚å­¦ä¹ ä»»åŠ¡ç‰¹å®šç‰¹å¾ï¼Œå¯ä»¥å˜åŒ–å¤§ä¸€äº›

**Q6: å¦‚ä½•é˜²æ­¢BERTå¾®è°ƒè¿‡æ‹Ÿåˆï¼Ÿ**

A: å¤šç§ç­–ç•¥ï¼š
1. **Early Stopping**ï¼šç›‘æ§éªŒè¯é›†ï¼Œæ€§èƒ½ä¸æå‡æ—¶åœæ­¢
2. **Dropout**ï¼šä¿æŒ0.1-0.3çš„dropout
3. **æ•°æ®å¢å¼º**ï¼šEDAã€å›è¯‘ã€Mixupç­‰
4. **æ­£åˆ™åŒ–**ï¼šL2 weight decayï¼ˆ0.01ï¼‰
5. **å‡å°‘è®­ç»ƒè½®æ•°**ï¼šBERTé€šå¸¸2-4ä¸ªepochè¶³å¤Ÿ
6. **å¯¹æŠ—è®­ç»ƒ**ï¼šFGM/PGDå¢å¼ºé²æ£’æ€§
7. **å¢åŠ æ•°æ®**ï¼šä¸»åŠ¨å­¦ä¹ ã€å¼±ç›‘ç£ç­‰

**Q7: Layer-wise Learning Rate Decayçš„åŸç†ï¼Ÿ**

A:
- ä¸åŒå±‚å­¦åˆ°çš„ç‰¹å¾æŠ½è±¡ç¨‹åº¦ä¸åŒï¼š
  - åº•å±‚ï¼ˆé è¿‘è¾“å…¥ï¼‰ï¼šé€šç”¨ç‰¹å¾ï¼ˆè¯­æ³•ã€è¯æ€§ç­‰ï¼‰
  - é¡¶å±‚ï¼ˆé è¿‘è¾“å‡ºï¼‰ï¼šä»»åŠ¡ç‰¹å®šç‰¹å¾
- åº•å±‚ç”¨å°å­¦ä¹ ç‡ä¿ç•™é€šç”¨çŸ¥è¯†
- é¡¶å±‚ç”¨å¤§å­¦ä¹ ç‡å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡
- å…¸å‹è®¾ç½®ï¼šlr_layer_i = base_lr Ã— decay^(L-i)ï¼Œdecay=0.95

### 8.3 çŸ¥è¯†è’¸é¦ç±»

**Q8: ä¸ºä»€ä¹ˆè½¯æ ‡ç­¾æ¯”ç¡¬æ ‡ç­¾åŒ…å«æ›´å¤šä¿¡æ¯ï¼Ÿ**

A:
```python
ç¡¬æ ‡ç­¾: [0, 0, 1, 0, 0]  # åªå‘Šè¯‰æ­£ç¡®ç±»åˆ«
è½¯æ ‡ç­¾: [0.01, 0.05, 0.85, 0.06, 0.03]  # åŒ…å«ç±»é—´ç›¸ä¼¼åº¦
```
- è½¯æ ‡ç­¾çš„éç›®æ ‡ç±»æ¦‚ç‡åŒ…å«"æš—çŸ¥è¯†"
- ä¾‹å¦‚ï¼šçŒ«çš„å›¾ç‰‡ï¼Œç‹—çš„æ¦‚ç‡é«˜äºæ±½è½¦ï¼Œè¯´æ˜çŒ«ç‹—ç›¸ä¼¼
- è¿™ç§ç›¸ä¼¼åº¦ä¿¡æ¯æœ‰åŠ©äºå­¦ç”Ÿæ¨¡å‹å­¦ä¹ 

**Q9: æ¸©åº¦å‚æ•°Tçš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ**

A:
```python
q_i = exp(z_i/T) / Î£ exp(z_j/T)
```
- T=1ï¼šæ ‡å‡†softmax
- T>1ï¼šåˆ†å¸ƒå˜å¹³æ»‘ï¼Œ"æš—çŸ¥è¯†"æ›´æ˜æ˜¾
  - [2, 1, 0.1] â†’ T=1: [0.7, 0.2, 0.1]
  - [2, 1, 0.1] â†’ T=5: [0.4, 0.35, 0.25]ï¼ˆæ›´å¹³æ»‘ï¼‰
- TÂ²ç”¨äºè¡¥å¿æ¢¯åº¦ç¼©æ”¾
- è®­ç»ƒæ—¶é«˜æ¸©ï¼Œæ¨ç†æ—¶T=1

**Q10: TinyBERTç›¸æ¯”DistilBERTæœ‰ä»€ä¹ˆæ”¹è¿›ï¼Ÿ**

A:
| æ–¹é¢ | DistilBERT | TinyBERT |
|------|------------|----------|
| è’¸é¦ç›®æ ‡ | è¾“å‡º+ä¸­é—´å±‚ | è¾“å‡º+ä¸­é—´å±‚+æ³¨æ„åŠ›+åµŒå…¥ |
| é˜¶æ®µ | å•é˜¶æ®µ | ä¸¤é˜¶æ®µï¼ˆé€šç”¨+ä»»åŠ¡ï¼‰ |
| æ•°æ®å¢å¼º | æ—  | ä»»åŠ¡é˜¶æ®µå¤§é‡å¢å¼º |
| å‹ç¼©æ¯” | 2Ã— | 7.5Ã— |
| æ€§èƒ½ | ~97% | ~96% |

TinyBERTé€šè¿‡æ›´å…¨é¢çš„è’¸é¦å’Œä¸¤é˜¶æ®µè®­ç»ƒå®ç°æ›´é«˜å‹ç¼©æ¯”ã€‚

### 8.4 å·¥ç¨‹å®è·µç±»

**Q11: ONNXç›¸æ¯”PyTorchéƒ¨ç½²æœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ**

A:
1. **è·¨å¹³å°**ï¼šä¸€æ¬¡è½¬æ¢ï¼Œå¤šå¤„éƒ¨ç½²ï¼ˆäº‘ç«¯ã€è¾¹ç¼˜ã€ç§»åŠ¨ï¼‰
2. **æ€§èƒ½ä¼˜åŒ–**ï¼šå›¾ä¼˜åŒ–ã€ç®—å­èåˆã€é‡åŒ–
3. **æ— ä¾èµ–**ï¼šä¸éœ€è¦å®‰è£…PyTorchï¼Œæ¨¡å‹æ›´å°
4. **æ¨ç†åŠ é€Ÿ**ï¼šONNX Runtimeä¸“é—¨ä¼˜åŒ–æ¨ç†
5. **ç¡¬ä»¶åŠ é€Ÿ**ï¼šæ›´å¥½æ”¯æŒTensorRTã€OpenVINOç­‰
6. **æ ‡å‡†åŒ–**ï¼šç»Ÿä¸€çš„æ¨¡å‹æ ¼å¼ä¾¿äºç®¡ç†

**Q12: åŠ¨æ€é‡åŒ–å’Œé™æ€é‡åŒ–çš„åŒºåˆ«ï¼Ÿ**

A:
| æ–¹é¢ | åŠ¨æ€é‡åŒ– | é™æ€é‡åŒ– |
|------|----------|----------|
| **æƒé‡** | INT8 | INT8 |
| **æ¿€æ´»** | FP32ï¼ˆè¿è¡Œæ—¶é‡åŒ–ï¼‰| INT8ï¼ˆé¢„å…ˆé‡åŒ–ï¼‰|
| **æ ¡å‡†æ•°æ®** | ä¸éœ€è¦ | éœ€è¦ |
| **ç²¾åº¦** | è¾ƒé«˜ï¼ˆ~1%æŸå¤±ï¼‰| ç¨ä½ï¼ˆ~2%æŸå¤±ï¼‰|
| **é€Ÿåº¦** | 2-3Ã— | 3-4Ã— |
| **é€‚ç”¨åœºæ™¯** | å†…å­˜å—é™ | CPUæ¨ç† |

é™æ€é‡åŒ–éœ€è¦æ ¡å‡†æ•°æ®ç¡®å®šæ¿€æ´»å€¼çš„é‡åŒ–èŒƒå›´ã€‚

**Q13: å¦‚ä½•é€‰æ‹©BERTçš„åºåˆ—é•¿åº¦ï¼Ÿ**

A: æƒè¡¡å› ç´ ï¼š
- **è®¡ç®—å¤æ‚åº¦**ï¼šO(nÂ²)ï¼Œé•¿åº¦åŠ å€â†’æ—¶é—´Ã—4
- **ä»»åŠ¡éœ€æ±‚**ï¼š
  - åˆ†ç±»ä»»åŠ¡ï¼š128é€šå¸¸è¶³å¤Ÿ
  - QAä»»åŠ¡ï¼š384-512
  - é•¿æ–‡æœ¬ï¼š512æˆ–åˆ†æ®µå¤„ç†
- **å†…å­˜é™åˆ¶**ï¼š
  - GPUæ˜¾å­˜æœ‰é™ï¼Œé•¿åºåˆ—éœ€è¦å°batch size
  - å¯ä»¥ç”¨æ¢¯åº¦ç´¯ç§¯æ¨¡æ‹Ÿå¤§batch
- **æ€§èƒ½æ”¶ç›Š**ï¼š
  - ç»Ÿè®¡æ•°æ®é›†ï¼Œå¦‚90%æ ·æœ¬<128ï¼Œç”¨128
  - å¤ªé•¿çš„åºåˆ—paddingæµªè´¹ï¼Œä¸”å¯èƒ½å¼•å…¥å™ªå£°

**Q14: BERTæ¨ç†åŠ é€Ÿæœ‰å“ªäº›æ–¹æ³•ï¼Ÿ**

A: å¤šå±‚æ¬¡ä¼˜åŒ–ï¼š
1. **æ¨¡å‹å±‚é¢**ï¼š
   - çŸ¥è¯†è’¸é¦ï¼šTinyBERTã€DistilBERT
   - æ¨¡å‹å‰ªæï¼šå»é™¤ä¸é‡è¦çš„æ³¨æ„åŠ›å¤´ã€å±‚
   - é‡åŒ–ï¼šINT8ã€Mixed Precision
2. **æ¨ç†å¼•æ“**ï¼š
   - ONNX Runtime
   - TensorRTï¼ˆNVIDIA GPUï¼‰
   - OpenVINOï¼ˆIntel CPUï¼‰
3. **ç®—æ³•ä¼˜åŒ–**ï¼š
   - æ‰¹å¤„ç†ï¼šå¢å¤§batch size
   - åºåˆ—æˆªæ–­ï¼šåªä¿ç•™å…³é”®éƒ¨åˆ†
   - ç¼“å­˜ï¼šç›¸åŒè¾“å…¥å¤ç”¨ç»“æœ
4. **ç¡¬ä»¶**ï¼š
   - GPUå¹¶è¡Œ
   - ä¸“ç”¨AIèŠ¯ç‰‡ï¼ˆTPUã€NPUï¼‰

### 8.5 æ·±åº¦ç†è§£ç±»

**Q15: Self-Attentionçš„æ—¶é—´å¤æ‚åº¦ä¸ºä»€ä¹ˆæ˜¯O(nÂ²)ï¼Ÿèƒ½å¦ä¼˜åŒ–ï¼Ÿ**

A: 
**å¤æ‚åº¦åˆ†æï¼š**
```python
Q @ K^T: (n, d) @ (d, n) = (n, n)  # O(nÂ²d)
Softmax: (n, n)                     # O(nÂ²)
@ V: (n, n) @ (n, d) = (n, d)      # O(nÂ²d)
æ€»è®¡: O(nÂ²d)
```

**ä¼˜åŒ–æ–¹æ³•ï¼š**
1. **Sparse Attention**ï¼ˆLongformerï¼‰ï¼šåªå…³æ³¨å±€éƒ¨+å…¨å±€ï¼ŒO(n)
2. **Linformer**ï¼šä½ç§©è¿‘ä¼¼ï¼ŒO(n)
3. **Performer**ï¼šæ ¸æ–¹æ³•è¿‘ä¼¼ï¼ŒO(n)
4. **Flash Attention**ï¼šIOä¼˜åŒ–ï¼Œå¤æ‚åº¦ä¸å˜ä½†å®é™…æ›´å¿«

**Q16: BERTçš„[CLS]ä¸ºä»€ä¹ˆèƒ½ä»£è¡¨æ•´ä¸ªå¥å­ï¼Ÿ**

A:
- [CLS]åœ¨ç¬¬ä¸€ä¸ªä½ç½®ï¼Œé€šè¿‡Self-Attentionå¯ä»¥å…³æ³¨æ‰€æœ‰token
- åœ¨é¢„è®­ç»ƒNSPä»»åŠ¡ä¸­ï¼Œ[CLS]è¢«è®­ç»ƒç”¨äºå¥å­çº§åˆ«åˆ†ç±»
- ç»è¿‡å¤šå±‚Transformerï¼Œ[CLS]èšåˆäº†å…¨å±€ä¿¡æ¯
- ä½†ä¹Ÿæœ‰ç ”ç©¶è¡¨æ˜å¹³å‡æ± åŒ–æ‰€æœ‰tokenæ•ˆæœç±»ä¼¼ç”šè‡³æ›´å¥½
- [CLS]æ›´å¤šæ˜¯ä¸€ç§çº¦å®šä¿—æˆçš„é€‰æ‹©

**Q17: BERTé¢„è®­ç»ƒä¸ºä»€ä¹ˆæœ‰æ•ˆï¼Ÿè¿ç§»å­¦ä¹ çš„æœ¬è´¨æ˜¯ä»€ä¹ˆï¼Ÿ**

A:
**ä¸ºä»€ä¹ˆæœ‰æ•ˆï¼š**
- å¤§è§„æ¨¡æ•°æ®å­¦åˆ°é€šç”¨è¯­è¨€çŸ¥è¯†ï¼ˆè¯­æ³•ã€è¯­ä¹‰ã€å¸¸è¯†ï¼‰
- ä¸‹æ¸¸ä»»åŠ¡æ•°æ®å°‘ï¼Œä»å¤´è®­ç»ƒå®¹æ˜“è¿‡æ‹Ÿåˆ
- é¢„è®­ç»ƒæä¾›å¥½çš„åˆå§‹åŒ–ï¼ŒåŠ é€Ÿæ”¶æ•›

**è¿ç§»å­¦ä¹ æœ¬è´¨ï¼š**
- åº•å±‚ç‰¹å¾é€šç”¨ï¼ˆè·¨ä»»åŠ¡å…±äº«ï¼‰
- é¡¶å±‚ç‰¹å¾ä»»åŠ¡ç‰¹å®š
- é¢„è®­ç»ƒå­¦é€šç”¨ï¼Œå¾®è°ƒå­¦ç‰¹å®š
- ç±»æ¯”è®¡ç®—æœºè§†è§‰ï¼šè¾¹ç¼˜æ£€æµ‹â†’çº¹ç†â†’éƒ¨ä»¶â†’ç‰©ä½“

**ä¸ºä»€ä¹ˆNLPæ¯”CVæ›´éœ€è¦é¢„è®­ç»ƒï¼š**
- è¯çš„è¡¨ç¤ºé«˜åº¦ä¾èµ–ä¸Šä¸‹æ–‡
- è¯­è¨€çš„ç»„åˆæ€§æ›´å¼º
- NLPæ•°æ®æ ‡æ³¨æˆæœ¬æ›´é«˜

**Q18: Transformerä¸ºä»€ä¹ˆè¦ç”¨Multi-Headè€Œä¸æ˜¯Single-Headï¼Ÿ**

A:
**å¤šå¤´çš„ä¼˜åŠ¿ï¼š**
1. **å¤šæ ·æ€§**ï¼šä¸åŒå¤´å…³æ³¨ä¸åŒæ¨¡å¼
   - è¯­æ³•å…³ç³»ï¼ˆä¸»è°“å®¾ï¼‰
   - è¯­ä¹‰ç›¸ä¼¼åº¦
   - ä½ç½®å…³ç³»
2. **å­ç©ºé—´åˆ†è§£**ï¼š
   - 768ç»´â†’12å¤´Ã—64ç»´
   - æ¯ä¸ªå¤´åœ¨ä½ç»´å­ç©ºé—´å­¦ä¹ 
   - ç±»ä¼¼CNNçš„å¤šä¸ªfilter
3. **é›†æˆæ•ˆåº”**ï¼šå¤šä¸ªå¤´çš„é¢„æµ‹æ›´é²æ£’
4. **è¡¨è¾¾èƒ½åŠ›**ï¼šå‚æ•°é‡ç›¸åŒæ—¶ï¼Œå¤šå¤´è¡¨è¾¾åŠ›æ›´å¼º

**Q19: ä¸ºä»€ä¹ˆBERTç”¨GELUè€Œä¸æ˜¯ReLUï¼Ÿ**

A:
```python
ReLU(x) = max(0, x)      # ç¡¬æˆªæ–­
GELU(x) = xÂ·Î¦(x)          # å¹³æ»‘
```

**GELUä¼˜åŠ¿ï¼š**
- å¹³æ»‘å¯å¾®ï¼Œä¼˜åŒ–æ›´ç¨³å®š
- è´Ÿå€¼åŒºåŸŸæœ‰å°æ¢¯åº¦ï¼ˆä¸åƒReLUå®Œå…¨ä¸º0ï¼‰
- éšæœºæ­£åˆ™åŒ–æ•ˆæœï¼šå¼•å…¥æ¦‚ç‡æ€§
- å®éªŒè¯æ˜Transformerä¸­æ•ˆæœæ›´å¥½
- GPTã€BERTç­‰ç°ä»£æ¨¡å‹æ ‡é…

**Q20: å¦‚ä½•ç†è§£BERTå­¦åˆ°çš„è¡¨ç¤ºï¼Ÿå¯è§†åŒ–æ–¹æ³•æœ‰å“ªäº›ï¼Ÿ**

A:
**ç†è§£æ–¹æ³•ï¼š**
1. **æ³¨æ„åŠ›å¯è§†åŒ–**ï¼š
   - ç»˜åˆ¶attention matrixçƒ­åŠ›å›¾
   - è§‚å¯Ÿå“ªäº›è¯å…³æ³¨å“ªäº›è¯
   - ä¸åŒå±‚ã€ä¸åŒå¤´å­¦åˆ°çš„æ¨¡å¼

2. **æ¢æµ‹ä»»åŠ¡ï¼ˆProbingï¼‰**ï¼š
   - å†»ç»“BERTï¼Œè®­ç»ƒç®€å•åˆ†ç±»å™¨
   - æµ‹è¯•æ˜¯å¦å­¦åˆ°å¥æ³•ï¼ˆè¯æ€§ã€ä¾å­˜ï¼‰
   - æµ‹è¯•æ˜¯å¦å­¦åˆ°è¯­ä¹‰ï¼ˆNERã€SRLï¼‰

3. **é™ç»´å¯è§†åŒ–**ï¼š
   - t-SNE/UMAPé™ç»´åˆ°2D
   - è§‚å¯Ÿè¯å‘é‡èšç±»
   - ç›¸ä¼¼è¯åº”è¯¥èšåœ¨ä¸€èµ·

4. **å¯¹æŠ—æ ·æœ¬**ï¼š
   - æ‰¾åˆ°æœ€å°æ‰°åŠ¨å¯¼è‡´é¢„æµ‹æ”¹å˜
   - æ­ç¤ºæ¨¡å‹å…³æ³¨çš„å…³é”®ç‰¹å¾

**å‘ç°ï¼š**
- ä½å±‚å­¦è¯­æ³•ï¼Œé«˜å±‚å­¦è¯­ä¹‰
- ä¸åŒå¤´å…³æ³¨ä¸åŒè¯­è¨€å­¦ç‰¹æ€§
- ä½ç½®é å‰çš„å±‚æ›´é€šç”¨

---

## é™„å½•

### A. å¸¸ç”¨èµ„æº

**è®ºæ–‡ï¼š**
- BERT: [arXiv:1810.04805](https://arxiv.org/abs/1810.04805)
- TinyBERT: [arXiv:1909.10351](https://arxiv.org/abs/1909.10351)
- DistilBERT: [arXiv:1910.01108](https://arxiv.org/abs/1910.01108)
- Attention is All You Need: [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)

**ä»£ç åº“ï¼š**
- Hugging Face Transformers: https://github.com/huggingface/transformers
- TinyBERT: https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/TinyBERT
- ONNX Runtime: https://github.com/microsoft/onnxruntime

**æ•™ç¨‹ï¼š**
- The Illustrated BERT: http://jalammar.github.io/illustrated-bert/
- The Annotated Transformer: http://nlp.seas.harvard.edu/annotated-transformer/

### B. æœ¯è¯­å¯¹ç…§è¡¨

| è‹±æ–‡ | ä¸­æ–‡ | ç¼©å†™ |
|------|------|------|
| Masked Language Model | æ©ç è¯­è¨€æ¨¡å‹ | MLM |
| Next Sentence Prediction | ä¸‹ä¸€å¥é¢„æµ‹ | NSP |
| Self-Attention | è‡ªæ³¨æ„åŠ› | - |
| Multi-Head Attention | å¤šå¤´æ³¨æ„åŠ› | MHA |
| Feed Forward Network | å‰é¦ˆç¥ç»ç½‘ç»œ | FFN |
| Knowledge Distillation | çŸ¥è¯†è’¸é¦ | KD |
| Fine-tuning | å¾®è°ƒ | - |
| Pretraining | é¢„è®­ç»ƒ | - |
| Token Embedding | è¯åµŒå…¥ | - |
| Position Embedding | ä½ç½®åµŒå…¥ | - |
| Segment Embedding | æ®µåµŒå…¥ | - |

### C. æ•°å­¦ç¬¦å·è¯´æ˜

| ç¬¦å· | å«ä¹‰ |
|------|------|
| Q, K, V | Query, Key, ValueçŸ©é˜µ |
| d_k, d_v | Keyå’ŒValueçš„ç»´åº¦ |
| h | æ³¨æ„åŠ›å¤´æ•° |
| L | Transformerå±‚æ•° |
| H | éšè—å±‚ç»´åº¦ |
| n | åºåˆ—é•¿åº¦ |
| T | æ¸©åº¦å‚æ•° |
| Î± | æŸå¤±æƒé‡ |
| W | æƒé‡çŸ©é˜µ |

---

**æ–‡æ¡£ç‰ˆæœ¬ï¼š** v1.0  
**æœ€åæ›´æ–°ï¼š** 2024å¹´  
**ä½œè€…ï¼š** AI Assistant  
**è®¸å¯ï¼š** CC BY-NC-SA 4.0

---

## å­¦ä¹ å»ºè®®

1. **å¾ªåºæ¸è¿›**ï¼šå…ˆç†è§£TransformeråŸºç¡€ï¼Œå†å­¦BERTï¼Œæœ€åæ·±å…¥è’¸é¦å’Œéƒ¨ç½²
2. **åŠ¨æ‰‹å®è·µ**ï¼šè¿è¡ŒHugging Faceä»£ç ï¼Œå¾®è°ƒä¸€ä¸ªåˆ†ç±»æ¨¡å‹
3. **è¯»æºç **ï¼šç†è§£Transformersåº“çš„å®ç°ç»†èŠ‚
4. **åšå®éªŒ**ï¼šå¯¹æ¯”ä¸åŒè¶…å‚æ•°ã€ä¸åŒè’¸é¦æ–¹æ³•çš„æ•ˆæœ
5. **å…³æ³¨å‰æ²¿**ï¼šè¿½è¸ªæœ€æ–°ç ”ç©¶ï¼ˆBERTâ†’RoBERTaâ†’ELECTRAâ†’DeBERTaâ†’...ï¼‰

**é¢è¯•å‡†å¤‡é‡ç‚¹ï¼š**
- âœ… BERTæ ¸å¿ƒåŸç†ï¼ˆTransformerã€MLMã€NSPï¼‰
- âœ… å¾®è°ƒæŠ€å·§ï¼ˆå­¦ä¹ ç‡ã€é˜²è¿‡æ‹Ÿåˆï¼‰
- âœ… çŸ¥è¯†è’¸é¦åŸç†ï¼ˆè½¯æ ‡ç­¾ã€æ¸©åº¦ï¼‰
- âœ… å·¥ç¨‹ä¼˜åŒ–ï¼ˆé‡åŒ–ã€ONNXã€éƒ¨ç½²ï¼‰
- âœ… æ·±åº¦ç†è§£ï¼ˆå¤æ‚åº¦ã€å¯¹æ¯”åˆ†æï¼‰

ç¥å­¦ä¹ é¡ºåˆ©ï¼ğŸš€

