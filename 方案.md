# TinyBERT 多轮对话意图识别 —— 单一推荐、可实现的完整工程方案（专业版）

> 目标：使用 **TinyBERT（4layer，任务蒸馏后）** 构建生产级的 **0-3 轮多轮对话意图分类器**。方案包括：数据规范、模型架构、Teacher 训练、蒸馏流程（logits、hidden、attention 对齐）、学生微调、评估、导出与部署。

---

## 目录
- [一、TinyBERT原理详解](#一tinybert原理详解)
- [二、任务与数据规范](#二任务与数据规范)
- [三、模型架构详解](#三模型架构详解)
- [四、数据预处理实现](#四数据预处理实现)
- [五、教师模型训练](#五教师模型teacher训练)
- [六、任务蒸馏实现](#六任务蒸馏实现)
- [七、学生微调](#七学生微调)
- [八、评估与分析](#八评估与误差分析)
- [九、导出与部署](#九导出与部署)
- [十、完整训练流程](#十完整训练流程)
- [十一、超参数配置](#十一基线超参数)
- [十二、风险与注意事项](#十二风险与注意事项)

---

## 一、TinyBERT原理详解

### 1.1 什么是TinyBERT？

**TinyBERT** 是华为诺亚方舟实验室提出的轻量级BERT模型，通过**知识蒸馏（Knowledge Distillation）**技术将大型BERT模型压缩而成。

#### 核心优势
- **模型更小**：参数量减少约7.5倍（BERT-base 110M → TinyBERT 14.5M）
- **推理更快**：速度提升约9.4倍
- **性能保持**：在多个NLP任务上保持96%以上的BERT-base性能

#### 架构对比
```
BERT-base:        12层 × 768维 × 12头 = 110M参数
TinyBERT-6L:      6层  × 312维 × 12头 = 14.5M参数
TinyBERT-4L:      4层  × 312维 × 12头 = 14.5M参数  ← 本项目采用
```

### 1.2 知识蒸馏核心原理

知识蒸馏是将大模型（Teacher）的知识迁移到小模型（Student）的过程：

```
大模型（BERT-base 12层）
    ↓ 知识蒸馏（多层次对齐）
小模型（TinyBERT 6层）
```

**为什么有效？**
1. **软标签**：Teacher输出概率分布 [0.7, 0.2, 0.1] 比硬标签 [1, 0, 0] 包含更多类间关系信息
2. **中间表示**：学习Teacher的语义空间和注意力模式
3. **温度平滑**：放大小概率类别的学习信号

### 1.3 TinyBERT的三层对齐策略

TinyBERT进行**多层次知识迁移**：

```
Teacher (BERT-base 12层)              Student (TinyBERT 4层)
├─ Embedding层                        ├─ Embedding层
│  └─ 768维                           │  └─ 312维 (投影对齐)
│                                     │
├─ Transformer层 0-11                 ├─ Transformer层 0-3
│  ├─ Hidden States (768)             │  ├─ Hidden States (312)
│  └─ Attention Maps (12头×L×L)       │  └─ Attention Maps (12头×L×L)
│                                     │
└─ 预测层 (Logits)                    └─ 预测层 (Logits)

层映射策略（4层Student）：
Student层0 ← Teacher层2  (embedding + 前3层)
Student层1 ← Teacher层5  (第4-6层)
Student层2 ← Teacher层8  (第7-9层)
Student层3 ← Teacher层11 (第10-12层)
```

#### 对齐1：Attention对齐
```python
L_attn = MSE(Student_Attention, Teacher_Attention)
```
- **目的**：学习Teacher的注意力模式（哪些词之间关系重要）
- **注意力矩阵**：(num_heads, seq_len, seq_len)
- **层映射**：Student第i层 → Teacher第(i×3+2)层 (4层Student情况)

#### 对齐2：Hidden States对齐
```python
L_hidden = MSE(W_h · Student_Hidden, Teacher_Hidden)
```
- **目的**：学习Teacher的语义表示空间
- **W_h**：线性投影矩阵（312 → 768），对齐维度差异
- **对齐位置**：每个Transformer层的输出

#### 对齐3：Logits对齐（软标签）
```python
L_kd = KL_Divergence(
    softmax(Student_Logits / T),
    softmax(Teacher_Logits / T)
) × T²
```
- **目的**：学习Teacher的决策边界
- **温度T**：平滑概率分布（T=3.0时，[0.8,0.15,0.05]更平滑）
- **T²补偿**：抵消温度对梯度的缩放影响

### 1.4 两阶段蒸馏策略

TinyBERT采用**两阶段蒸馏**：

1. **通用蒸馏（General Distillation）**
   - 在大规模无标注语料上预训练
   - 学习通用语言表示
   - 官方提供预训练checkpoint

2. **任务蒸馏（Task-specific Distillation）** ← **本项目重点**
   - 在特定任务（意图分类）上蒸馏
   - 使用有标注对话数据
   - 同时使用真实标签和Teacher输出

### 1.5 本方案简要结论
采用**序列拼接编码 + TinyBERT 4层学生**，教师使用**BERT-base-chinese（带分类头）**做任务微调监督。多轮对话通过说话人标记拼接成单一序列，BERT一次性编码整段对话。蒸馏包含 **logits蒸馏、hidden states对齐、注意力对齐**，然后对student进行任务微调以优化下游指标并导出为ONNX供生产使用。

---

## 二、任务与数据规范
### 2.1 输入输出
- 输入：1–4 轮对话（0~3轮），每轮 `{speaker: user|system, text: str}`。说话人信息通过文本标记 `[USER]`/`[SYSTEM]` 编码。
- 输出：单标签意图类别（0~11），表示整个对话的最终意图。

### 2.2 标注原则
- 若后续轮次明确提出技能调用，以最后用户目标为准。空间 vs 视觉：必须移动机器人 → 空间事务处理；无需移动即可回答 → 视觉问答。无法确定 → Other。

### 2.3 数据格式
```json
{ "dialogue_id": "d1", "turns": [{"speaker":"user","text":"今天天气如何？"}, ...], "label": 2 }
```

### 2.4 划分
- Train/Dev/Test = 80/10/10，保证对话独立。每类至少 500 条，稀缺类通过增强生成。

---

## 三、模型架构详解

### 3.1 总体架构流程（序列拼接方式）

```
输入对话：
[
  {speaker: "user", text: "今天下午提醒我喝水"},
  {speaker: "system", text: "好的，需要几点提醒？"},
  {speaker: "user", text: "下午3点"},
]
        ↓
    添加说话人标记并拼接
        ↓
"[USER] 今天下午提醒我喝水 [SYSTEM] 好的，需要几点提醒？ [USER] 下午3点"
        ↓
    BERT Tokenization (一次性编码整段对话)
        ↓
[CLS] [ USER ] 今 天 ... 3 点 [SEP] [PAD] ...
        ↓
BERT Encoder (Teacher: 12层768维 / Student: 4层312维)
    全局自注意力，任意token可交互
        ↓
取 [CLS] token 表示
        ↓
Classification Head (Dropout + Linear)
        ↓
意图类别 (0-11)
```

**核心优势**：
- ✅ **全局注意力**：所有轮次的token可直接交互，捕获长距离依赖
- ✅ **简单高效**：一次BERT调用，无需额外的聚合模块
- ✅ **符合预训练**：与BERT预训练时的输入形式一致
- ✅ **灵活支持**：自动适配1-4轮的变长对话

### 3.2 模型组件详解

#### 3.2.1 BERT Encoder
- **Teacher**: `bert-base-chinese` (12层, 768维, ~102M参数)
- **Student**: `TinyBERT_General_4L_312D` (4层, 312维, ~14.5M参数)
- **功能**: 编码拼接后的完整对话序列
- **输入**: 最大序列长度 320 tokens（足够容纳4轮对话）
- **输出**: [CLS] token的hidden state作为对话表示

**为什么选择4层？**
- ✅ **更快推理**: 比6层快约33%
- ✅ **更小模型**: 参数量更少，部署更轻量
- ✅ **足够性能**: 对于意图分类任务，4层已能保持良好性能
- ✅ **更好蒸馏**: 12层→4层，层映射更清晰（每3层对应1层）

#### 3.2.2 说话人信息编码
```python
# 说话人信息通过文本标记嵌入
# 不使用独立的embedding层，而是让BERT学习标记的语义
dialogue_parts = []
for turn in turns:
    speaker = turn['speaker'].upper()  # USER or SYSTEM
    text = turn['text']
    dialogue_parts.append(f"[{speaker}] {text}")

full_dialogue = " ".join(dialogue_parts)
# 输出示例: "[USER] 订机票 [SYSTEM] 好的 [USER] 北京到上海"
```

**优势**：
- 说话人标记在tokenizer词表中（或作为未知词）
- BERT通过上下文学习说话人的语义差异
- 无需额外参数，更简洁

#### 3.2.3 Classification Head
```python
class MultiTurnDialogueClassifier(nn.Module):
    def __init__(self):
        self.encoder = BertModel.from_pretrained('bert-base-chinese')
        self.dropout = nn.Dropout(0.1)
        self.classifier = nn.Linear(768, 12)  # Teacher: 768维
    
    def forward(self, input_ids, attention_mask):
        outputs = self.encoder(input_ids, attention_mask)
        cls_output = outputs.last_hidden_state[:, 0, :]  # 取[CLS]
        logits = self.classifier(self.dropout(cls_output))
        return logits
```

**关键参数**：
- Teacher: 768 → 12
- Student: 312 → 12
- Dropout: 0.1

### 3.3 完整模型代码实现（序列拼接方式）

```python
import torch
import torch.nn as nn
from transformers import BertModel
from typing import Dict, Optional

class MultiTurnDialogueClassifier(nn.Module):
    """
    多轮对话意图分类器（序列拼接方式）
    支持Teacher（BERT-base）和Student（TinyBERT）
    """
    def __init__(
        self,
        encoder_name: str,
        num_labels: int = 12,
        max_seq_length: int = 320,  # 拼接后的总长度
        dropout: float = 0.1,
    ):
        """
        Args:
            encoder_name: 预训练模型名称 (bert-base-chinese / TinyBERT)
            num_labels: 意图类别数
            max_seq_length: 拼接后的最大序列长度
            dropout: Dropout概率
        """
        super().__init__()
        
        self.num_labels = num_labels
        self.max_seq_length = max_seq_length
        
        # 1. BERT编码器（处理拼接后的对话）
        self.encoder = BertModel.from_pretrained(encoder_name)
        self.hidden_size = self.encoder.config.hidden_size
        
        # 2. 分类头
        self.dropout = nn.Dropout(dropout)
        self.classifier = nn.Linear(self.hidden_size, num_labels)
        
        # 初始化分类器
        self._init_weights()
    
    def _init_weights(self):
        """初始化分类器权重"""
        nn.init.normal_(self.classifier.weight, std=0.02)
        nn.init.zeros_(self.classifier.bias)
    
    def forward(
        self,
        input_ids: torch.Tensor,
        attention_mask: torch.Tensor,
        speaker_ids: torch.Tensor,
        turn_ids: torch.Tensor,
        output_hidden_states: bool = False,
        output_attentions: bool = False
    ):
        """
        Args:
            input_ids: (batch_size, num_turns, seq_len)
            attention_mask: (batch_size, num_turns, seq_len)
            speaker_ids: (batch_size, num_turns) - 0=user, 1=system
            turn_ids: (batch_size, num_turns) - 0,1,2,3
            output_hidden_states: 是否输出hidden states（用于蒸馏）
            output_attentions: 是否输出attention（用于蒸馏）
        
        Returns:
            {
                'logits': (batch_size, num_labels),
                'turn_hidden_states': (batch_size, num_turns, hidden_size),
                'attentions': List of attention tensors (if requested)
            }
        """
        batch_size, num_turns, seq_len = input_ids.size()
        
        # Step 1: 展平batch，逐轮编码
        # (batch_size, num_turns, seq_len) -> (batch_size * num_turns, seq_len)
        input_ids_flat = input_ids.view(-1, seq_len)
        attention_mask_flat = attention_mask.view(-1, seq_len)
        
        # Step 2: 编码每一轮对话
        encoder_outputs = self.encoder(
            input_ids=input_ids_flat,
            attention_mask=attention_mask_flat,
            output_hidden_states=output_hidden_states,
            output_attentions=output_attentions,
            return_dict=True
        )
        
        # 取CLS向量作为每轮的表示: (batch_size * num_turns, hidden_size)
        turn_cls = encoder_outputs.last_hidden_state[:, 0, :]
        
        # Step 3: 重塑回多轮形状: (batch_size, num_turns, hidden_size)
        turn_cls = turn_cls.view(batch_size, num_turns, self.hidden_size)
        
        # Step 4: 添加speaker和turn position embeddings
        speaker_emb = self.speaker_embedding(speaker_ids)  # (B, T, H)
        turn_pos_emb = self.turn_position_embedding(turn_ids)  # (B, T, H)
        turn_repr = turn_cls + speaker_emb + turn_pos_emb
        
        # Step 5: Aggregation（多轮信息融合）
        # Transformer需要输入形状: (seq_len, batch, hidden)
        turn_repr_transposed = turn_repr.transpose(0, 1)  # (T, B, H)
        
        # 创建turn mask（忽略padding的轮次）
        turn_mask = (turn_ids == -1)  # padding的轮次id设为-1
        
        dialogue_repr = self.aggregator(
            turn_repr_transposed,
            src_key_padding_mask=turn_mask if turn_mask.any() else None
        )  # (T, B, H)
        
        # 取第一个位置作为对话级表示
        dialogue_cls = dialogue_repr[0]  # (B, H)
        
        # Step 6: 分类预测
        dialogue_cls = self.dropout(dialogue_cls)
        logits = self.classifier(dialogue_cls)  # (B, num_labels)
        
        # 返回结果
        result = {
            'logits': logits,
            'turn_hidden_states': turn_cls,  # 用于蒸馏的hidden对齐
        }
        
        if output_attentions:
            result['attentions'] = encoder_outputs.attentions
        
        if output_hidden_states:
            result['all_hidden_states'] = encoder_outputs.hidden_states
        
        return result
    
    def get_encoder_parameters(self):
        """获取编码器参数（用于差异化学习率）"""
        return self.encoder.parameters()
    
    def get_task_parameters(self):
        """获取任务相关参数（aggregator + classifier）"""
        params = []
        params.extend(self.speaker_embedding.parameters())
        params.extend(self.turn_position_embedding.parameters())
        params.extend(self.aggregator.parameters())
        params.extend(self.classifier.parameters())
        return params
```

### 3.4 模型输入输出示例

```python
# 输入示例
batch = {
    'input_ids': torch.tensor([[[101, 1045, ...], [101, 2023, ...], ...]]),  # (B, T, L)
    'attention_mask': torch.tensor([[[1, 1, ...], [1, 1, ...], ...]]),
    'speaker_ids': torch.tensor([[0, 1, 0, 1]]),  # user, system, user, system
    'turn_ids': torch.tensor([[0, 1, 2, 3]]),
    'labels': torch.tensor([6])  # 意图类别
}

# 前向传播
outputs = model(
    input_ids=batch['input_ids'],
    attention_mask=batch['attention_mask'],
    speaker_ids=batch['speaker_ids'],
    turn_ids=batch['turn_ids'],
    output_attentions=True  # 蒸馏时需要
)

# 输出
print(outputs['logits'].shape)  # (batch_size, 12)
print(outputs['turn_hidden_states'].shape)  # (batch_size, 4, hidden_size)
```

---

## 四、数据预处理实现（序列拼接方式）

### 4.1 Dataset类实现

```python
import json
import torch
from torch.utils.data import Dataset
from transformers import BertTokenizer
from typing import List, Dict

class MultiTurnDialogueDataset(Dataset):
    """
    多轮对话数据集（序列拼接方式）
    将多轮对话拼接成单个序列，添加说话人标记
    """
    def __init__(
        self,
        data_path: str,
        tokenizer: BertTokenizer,
        max_seq_length: int = 320,  # 拼接后的总长度
        is_training: bool = True
    ):
        """
        Args:
            data_path: JSON数据文件路径
            tokenizer: BERT tokenizer
            max_seq_length: 拼接后的最大序列长度
            is_training: 是否训练模式
        """
        super().__init__()
        
        # 加载数据
        with open(data_path, 'r', encoding='utf-8') as f:
            self.data = json.load(f)
        
        self.tokenizer = tokenizer
        self.max_seq_length = max_seq_length
        self.is_training = is_training
        
        print(f"Loaded {len(self.data)} dialogues from {data_path}")
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        """
        返回一个对话样本的处理结果
        """
        item = self.data[idx]
        
        # 提取对话信息
        turns = item['turns']
        label = item['label']
        dialogue_id = item.get('dialogue_id', f'dia_{idx}')
        num_turns = len(turns)
        
        # 构建拼接文本，添加说话人标记
        # 格式: [CLS] [USER] turn1 [SEP] [SYSTEM] turn2 [SEP] ...
        dialogue_parts = []
        for turn in turns:
            speaker = turn['speaker'].upper()  # USER or SYSTEM
            text = turn['text']
            dialogue_parts.append(f"[{speaker}] {text}")
        
        # 用空格连接所有轮次
        full_dialogue = " ".join(dialogue_parts)
        
        # 一次性tokenize整个对话
        encoded = self.tokenizer(
            full_dialogue,
            max_length=self.max_seq_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoded['input_ids'].squeeze(0),  # (seq_len,)
            'attention_mask': encoded['attention_mask'].squeeze(0),  # (seq_len,)
            'token_type_ids': encoded.get('token_type_ids', torch.zeros_like(encoded['input_ids'])).squeeze(0),
            'labels': torch.tensor(label, dtype=torch.long),
            'dialogue_id': dialogue_id,
            'num_turns': num_turns
        }
        
        # Padding到max_turns（如果轮次不足）
        num_actual_turns = len(turns)
        while len(input_ids_list) < self.max_turns:
            # Padding轮次：全0的input_ids和attention_mask
            input_ids_list.append(torch.zeros(self.max_seq_length, dtype=torch.long))
            attention_mask_list.append(torch.zeros(self.max_seq_length, dtype=torch.long))
            speaker_ids_list.append(0)  # 默认user
            turn_ids_list.append(-1)  # 用-1标记padding轮次
        
        return {
            'input_ids': torch.stack(input_ids_list),  # (num_turns, seq_len)
            'attention_mask': torch.stack(attention_mask_list),
            'speaker_ids': torch.tensor(speaker_ids_list, dtype=torch.long),
            'turn_ids': torch.tensor(turn_ids_list, dtype=torch.long),
            'labels': torch.tensor(label, dtype=torch.long),
            'dialogue_id': dialogue_id,
            'num_turns': num_actual_turns  # 实际轮次数（用于评估）
        }


def create_dataloaders(
    train_path: str,
    dev_path: str,
    test_path: str,
    tokenizer: BertTokenizer,
    batch_size: int = 32,
    max_turns: int = 4,
    max_seq_length: int = 80,
    num_workers: int = 4
):
    """
    创建训练、验证、测试数据加载器
    """
    from torch.utils.data import DataLoader
    
    # 创建数据集
    train_dataset = MultiTurnDialogueDataset(
        train_path, tokenizer, max_turns, max_seq_length, is_training=True
    )
    dev_dataset = MultiTurnDialogueDataset(
        dev_path, tokenizer, max_turns, max_seq_length, is_training=False
    )
    test_dataset = MultiTurnDialogueDataset(
        test_path, tokenizer, max_turns, max_seq_length, is_training=False
    )
    
    # 创建DataLoader
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True
    )
    
    dev_loader = DataLoader(
        dev_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True
    )
    
    print(f"Train batches: {len(train_loader)}")
    print(f"Dev batches: {len(dev_loader)}")
    print(f"Test batches: {len(test_loader)}")
    
    return train_loader, dev_loader, test_loader
```

### 4.2 数据预处理示例

```python
from transformers import BertTokenizer

# 初始化tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')

# 可选：添加特殊token（如果需要）
# special_tokens = {'additional_special_tokens': ['[USR]', '[SYS]']}
# tokenizer.add_special_tokens(special_tokens)

# 创建数据加载器
train_loader, dev_loader, test_loader = create_dataloaders(
    train_path='data/train.json',
    dev_path='data/dev.json',
    test_path='data/test.json',
    tokenizer=tokenizer,
    batch_size=32,
    max_turns=4,
    max_seq_length=80
)

# 查看一个batch
for batch in train_loader:
    print("Input IDs shape:", batch['input_ids'].shape)  # (32, 4, 80)
    print("Attention mask shape:", batch['attention_mask'].shape)
    print("Speaker IDs shape:", batch['speaker_ids'].shape)  # (32, 4)
    print("Turn IDs shape:", batch['turn_ids'].shape)
    print("Labels shape:", batch['labels'].shape)  # (32,)
    break
```

### 4.3 数据统计与分析

```python
import numpy as np
from collections import Counter

def analyze_dataset(data_path: str):
    """分析数据集统计信息"""
    with open(data_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # 标签分布
    labels = [item['label'] for item in data]
    label_counts = Counter(labels)
    
    # 轮次分布
    num_turns = [len(item['turns']) for item in data]
    
    # 文本长度分布
    text_lengths = []
    for item in data:
        for turn in item['turns']:
            text_lengths.append(len(turn['text']))
    
    print(f"Total dialogues: {len(data)}")
    print(f"\nLabel distribution:")
    for label, count in sorted(label_counts.items()):
        print(f"  Label {label}: {count} ({count/len(data)*100:.1f}%)")
    
    print(f"\nTurn distribution:")
    print(f"  Mean: {np.mean(num_turns):.2f}")
    print(f"  Min: {np.min(num_turns)}, Max: {np.max(num_turns)}")
    
    print(f"\nText length distribution:")
    print(f"  Mean: {np.mean(text_lengths):.1f} chars")
    print(f"  95th percentile: {np.percentile(text_lengths, 95):.0f} chars")

# 分析各数据集
print("=== Training Set ===")
analyze_dataset('data/train.json')

print("\n=== Dev Set ===")
analyze_dataset('data/dev.json')

print("\n=== Test Set ===")
analyze_dataset('data/test.json')
```

---

### 5.1 Teacher配置

- **Backbone**: `bert-base-chinese` (12层, 768维)
- **架构**: 使用前面定义的`MultiTurnDialogueClassifier`
- **训练目标**: 在意图分类任务上达到最优性能

### 5.2 Teacher训练代码

```python
import torch
import torch.nn as nn
from torch.optim import AdamW
from transformers import get_linear_schedule_with_warmup
from sklearn.metrics import f1_score, classification_report
import numpy as np
from tqdm import tqdm

class TeacherTrainer:
    """Teacher模型训练器"""
    def __init__(
        self,
        model: nn.Module,
        train_loader,
        dev_loader,
        device: str = 'cuda',
        lr: float = 2e-5,
        weight_decay: float = 0.01,
        num_epochs: int = 5,
        warmup_ratio: float = 0.1,
        save_dir: str = 'checkpoints/teacher'
    ):
        self.model = model.to(device)
        self.train_loader = train_loader
        self.dev_loader = dev_loader
        self.device = device
        self.num_epochs = num_epochs
        self.save_dir = save_dir
        
        # 优化器（差异化学习率）
        encoder_params = list(model.get_encoder_parameters())
        task_params = list(model.get_task_parameters())
        
        self.optimizer = AdamW([
            {'params': encoder_params, 'lr': lr},
            {'params': task_params, 'lr': lr * 2}  # task层学习率更高
        ], weight_decay=weight_decay)
        
        # 学习率调度器
        total_steps = len(train_loader) * num_epochs
        warmup_steps = int(total_steps * warmup_ratio)
        self.scheduler = get_linear_schedule_with_warmup(
            self.optimizer,
            num_warmup_steps=warmup_steps,
            num_training_steps=total_steps
        )
        
        # 损失函数
        self.criterion = nn.CrossEntropyLoss()
        
        # 最佳模型跟踪
        self.best_f1 = 0.0
        
        print(f"Total training steps: {total_steps}")
        print(f"Warmup steps: {warmup_steps}")
    
    def train_epoch(self):
        """训练一个epoch"""
        self.model.train()
        total_loss = 0
        all_preds = []
        all_labels = []
        
        pbar = tqdm(self.train_loader, desc="Training")
        for batch in pbar:
            # 移动数据到device
            input_ids = batch['input_ids'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)
            speaker_ids = batch['speaker_ids'].to(self.device)
            turn_ids = batch['turn_ids'].to(self.device)
            labels = batch['labels'].to(self.device)
            
            # 前向传播
            outputs = self.model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                speaker_ids=speaker_ids,
                turn_ids=turn_ids
            )
            
            logits = outputs['logits']
            loss = self.criterion(logits, labels)
            
            # 反向传播
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
            self.optimizer.step()
            self.scheduler.step()
            
            # 统计
            total_loss += loss.item()
            preds = torch.argmax(logits, dim=-1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            
            pbar.set_postfix({'loss': f'{loss.item():.4f}'})
        
        avg_loss = total_loss / len(self.train_loader)
        train_f1 = f1_score(all_labels, all_preds, average='macro')
        
        return avg_loss, train_f1
    
    def evaluate(self):
        """在验证集上评估"""
        self.model.eval()
        total_loss = 0
        all_preds = []
        all_labels = []
        
        with torch.no_grad():
            for batch in tqdm(self.dev_loader, desc="Evaluating"):
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                speaker_ids = batch['speaker_ids'].to(self.device)
                turn_ids = batch['turn_ids'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    speaker_ids=speaker_ids,
                    turn_ids=turn_ids
                )
                
                logits = outputs['logits']
                loss = self.criterion(logits, labels)
                
                total_loss += loss.item()
                preds = torch.argmax(logits, dim=-1)
                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
        
        avg_loss = total_loss / len(self.dev_loader)
        macro_f1 = f1_score(all_labels, all_preds, average='macro')
        
        # 打印分类报告
        print("\nClassification Report:")
        print(classification_report(all_labels, all_preds, digits=4))
        
        return avg_loss, macro_f1
    
    def train(self):
        """完整训练流程"""
        import os
        os.makedirs(self.save_dir, exist_ok=True)
        
        for epoch in range(self.num_epochs):
            print(f"\n{'='*50}")
            print(f"Epoch {epoch + 1}/{self.num_epochs}")
            print(f"{'='*50}")
            
            # 训练
            train_loss, train_f1 = self.train_epoch()
            print(f"Train Loss: {train_loss:.4f}, Train Macro-F1: {train_f1:.4f}")
            
            # 评估
            dev_loss, dev_f1 = self.evaluate()
            print(f"Dev Loss: {dev_loss:.4f}, Dev Macro-F1: {dev_f1:.4f}")
            
            # 保存最佳模型
            if dev_f1 > self.best_f1:
                self.best_f1 = dev_f1
                save_path = os.path.join(self.save_dir, 'best_model.pt')
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'dev_f1': dev_f1,
                }, save_path)
                print(f"✓ Saved best model with F1: {dev_f1:.4f}")
        
        print(f"\nTraining completed. Best Dev F1: {self.best_f1:.4f}")


# 使用示例
def train_teacher():
    """训练Teacher模型"""
    from transformers import BertTokenizer
    
    # 1. 准备数据
    tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
    train_loader, dev_loader, test_loader = create_dataloaders(
        train_path='data/train.json',
        dev_path='data/dev.json',
        test_path='data/test.json',
        tokenizer=tokenizer,
        batch_size=32
    )
    
    # 2. 初始化模型
    teacher_model = MultiTurnDialogueClassifier(
        encoder_name='bert-base-chinese',
        num_labels=12,
        num_turns=4,
        max_seq_length=80
    )
    
    # 3. 训练
    trainer = TeacherTrainer(
        model=teacher_model,
        train_loader=train_loader,
        dev_loader=dev_loader,
        device='cuda',
        lr=2e-5,
        num_epochs=5
    )
    
    trainer.train()
    
    return teacher_model, tokenizer

# 运行训练
# teacher_model, tokenizer = train_teacher()
```

### 5.3 训练超参数

| 参数 | 值 | 说明 |
|------|-----|------|
| **学习率** | 2e-5 (encoder)<br>4e-5 (task层) | 差异化学习率 |
| **Batch Size** | 32 | 根据GPU显存调整 |
| **Epochs** | 5 | 早停防止过拟合 |
| **Warmup Ratio** | 0.1 | 前10%步数warmup |
| **Weight Decay** | 0.01 | L2正则化 |
| **梯度裁剪** | 1.0 | 防止梯度爆炸 |
| **优化器** | AdamW | 带解耦权重衰减 |

### 5.4 输出保存

训练完成后保存：
- **模型checkpoint**: `checkpoints/teacher/best_model.pt`
- **Tokenizer**: 用于Student训练
- **评估指标**: Dev Macro-F1, 分类报告

---

### 6.1 蒸馏原理详解

#### 6.1.1 蒸馏损失函数

TinyBERT使用**多层次蒸馏损失**：

```
L_total = λ_task·L_task + λ_kd·L_kd + λ_hidden·L_hidden + λ_attn·L_attn
```

**各项损失说明**：

1. **任务损失（L_task）**：真实标签的交叉熵
```python
   L_task = CrossEntropy(student_logits, true_labels)
```
   
2. **Logits蒸馏（L_kd）**：软标签知识蒸馏
```python
   L_kd = KL_Divergence(
       softmax(student_logits / T),
       softmax(teacher_logits / T)
   ) × T²
   ```
   
3. **Hidden States对齐（L_hidden）**：中间表示对齐
```python
   L_hidden = MSE(W_h · student_hidden, teacher_hidden)
   ```
   
4. **Attention对齐（L_attn）**：注意力模式对齐
   ```python
   L_attn = MSE(student_attention, teacher_attention)
   ```

#### 6.1.2 关键参数

| 参数 | 值 | 作用 |
|------|-----|------|
| **λ_task** | 1.0 | 保证准确率 |
| **λ_kd** | 1.0 | 学习决策边界 |
| **λ_hidden** | 0.5 | 学习语义表示 |
| **λ_attn** | 0.25 | 学习注意力模式 |
| **温度T** | 3.0 | 平滑概率分布 |

### 6.2 蒸馏训练器实现

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import AdamW
from transformers import get_linear_schedule_with_warmup
from sklearn.metrics import f1_score
from tqdm import tqdm

class DistillationTrainer:
    """
    知识蒸馏训练器
    实现TinyBERT的任务蒸馏策略
    """
    def __init__(
        self,
        teacher_model: nn.Module,
        student_model: nn.Module,
        train_loader,
        dev_loader,
        device: str = 'cuda',
        lr: float = 3e-5,
        num_epochs: int = 10,
        lambda_task: float = 1.0,
        lambda_kd: float = 1.0,
        lambda_hidden: float = 0.5,
        lambda_attn: float = 0.25,
        temperature: float = 3.0,
        save_dir: str = 'checkpoints/student'
    ):
        self.teacher = teacher_model.to(device).eval()  # Teacher冻结
        self.student = student_model.to(device)
        self.train_loader = train_loader
        self.dev_loader = dev_loader
        self.device = device
        self.num_epochs = num_epochs
        
        # 蒸馏超参数
        self.lambda_task = lambda_task
        self.lambda_kd = lambda_kd
        self.lambda_hidden = lambda_hidden
        self.lambda_attn = lambda_attn
        self.temperature = temperature
        
        self.save_dir = save_dir
        
        # 维度对齐层（TinyBERT 312维 → BERT 768维）
        teacher_hidden = teacher_model.hidden_size
        student_hidden = student_model.hidden_size
        
        if student_hidden != teacher_hidden:
            self.hidden_projection = nn.Linear(
                student_hidden, teacher_hidden
            ).to(device)
            nn.init.xavier_uniform_(self.hidden_projection.weight)
        else:
            self.hidden_projection = None
        
        # 优化器（包括projection层）
        params = list(student_model.parameters())
        if self.hidden_projection:
            params += list(self.hidden_projection.parameters())
        
        self.optimizer = AdamW(params, lr=lr, weight_decay=0.01)
        
        # 学习率调度器
        total_steps = len(train_loader) * num_epochs
        warmup_steps = int(total_steps * 0.1)
        self.scheduler = get_linear_schedule_with_warmup(
            self.optimizer, warmup_steps, total_steps
        )
        
        # 层映射（Student 6层 → Teacher 12层）
        self.layer_mapping = {0: 0, 1: 2, 2: 4, 3: 6, 4: 8, 5: 10}
        
        # 最佳模型跟踪
        self.best_f1 = 0.0
        
        print(f"Teacher hidden size: {teacher_hidden}")
        print(f"Student hidden size: {student_hidden}")
        print(f"Layer mapping: {self.layer_mapping}")
        print(f"Total steps: {total_steps}, Warmup: {warmup_steps}")
    
    def compute_distillation_loss(self, batch, labels):
        """
        计算蒸馏损失
        """
        # 1. Teacher前向（不计算梯度）
        with torch.no_grad():
            teacher_outputs = self.teacher(
                input_ids=batch['input_ids'],
                attention_mask=batch['attention_mask'],
                speaker_ids=batch['speaker_ids'],
                turn_ids=batch['turn_ids'],
                output_hidden_states=True,
                output_attentions=True
            )
        
        # 2. Student前向
        student_outputs = self.student(
            input_ids=batch['input_ids'],
            attention_mask=batch['attention_mask'],
            speaker_ids=batch['speaker_ids'],
            turn_ids=batch['turn_ids'],
            output_hidden_states=True,
            output_attentions=True
        )
        
        # 3. 任务损失（真实标签）
        loss_task = F.cross_entropy(student_outputs['logits'], labels)
        
        # 4. Logits蒸馏损失（软标签）
        T = self.temperature
        loss_kd = F.kl_div(
            F.log_softmax(student_outputs['logits'] / T, dim=-1),
            F.softmax(teacher_outputs['logits'] / T, dim=-1),
            reduction='batchmean'
        ) * (T * T)
        
        # 5. Hidden states对齐损失
        student_hidden = student_outputs['turn_hidden_states']  # (B, T, 312)
        teacher_hidden = teacher_outputs['turn_hidden_states']  # (B, T, 768)
        
        if self.hidden_projection:
            student_hidden_proj = self.hidden_projection(student_hidden)
        else:
            student_hidden_proj = student_hidden
        
        loss_hidden = F.mse_loss(student_hidden_proj, teacher_hidden)
        
        # 6. Attention对齐损失（层映射）
        loss_attn = 0
        student_attentions = student_outputs['attentions']
        teacher_attentions = teacher_outputs['attentions']
        
        for s_layer, t_layer in self.layer_mapping.items():
            if s_layer < len(student_attentions) and t_layer < len(teacher_attentions):
                s_attn = student_attentions[s_layer]  # (B*T, heads, L, L)
                t_attn = teacher_attentions[t_layer]
                loss_attn += F.mse_loss(s_attn, t_attn)
        
        loss_attn /= len(self.layer_mapping)
        
        # 7. 总损失
        total_loss = (
            self.lambda_task * loss_task +
            self.lambda_kd * loss_kd +
            self.lambda_hidden * loss_hidden +
            self.lambda_attn * loss_attn
        )
        
        return {
            'loss': total_loss,
            'loss_task': loss_task.item(),
            'loss_kd': loss_kd.item(),
            'loss_hidden': loss_hidden.item(),
            'loss_attn': loss_attn.item()
        }
    
    def train_epoch(self):
        """训练一个epoch"""
        self.student.train()
        if self.hidden_projection:
            self.hidden_projection.train()
        
        total_losses = {'loss': 0, 'loss_task': 0, 'loss_kd': 0, 
                       'loss_hidden': 0, 'loss_attn': 0}
        all_preds = []
        all_labels = []
        
        pbar = tqdm(self.train_loader, desc="Distillation")
        for batch in pbar:
            # 移动数据到device
            batch_data = {
                'input_ids': batch['input_ids'].to(self.device),
                'attention_mask': batch['attention_mask'].to(self.device),
                'speaker_ids': batch['speaker_ids'].to(self.device),
                'turn_ids': batch['turn_ids'].to(self.device)
            }
            labels = batch['labels'].to(self.device)
            
            # 计算蒸馏损失
            loss_dict = self.compute_distillation_loss(batch_data, labels)
            
            # 反向传播
            self.optimizer.zero_grad()
            loss_dict['loss'].backward()
            torch.nn.utils.clip_grad_norm_(self.student.parameters(), 1.0)
            self.optimizer.step()
            self.scheduler.step()
            
            # 统计
            for key in total_losses:
                if key in loss_dict:
                    total_losses[key] += loss_dict[key] if isinstance(loss_dict[key], float) else loss_dict[key].item()
            
            # 预测
            with torch.no_grad():
                student_outputs = self.student(
                    input_ids=batch_data['input_ids'],
                    attention_mask=batch_data['attention_mask'],
                    speaker_ids=batch_data['speaker_ids'],
                    turn_ids=batch_data['turn_ids']
                )
                preds = torch.argmax(student_outputs['logits'], dim=-1)
                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
            
            pbar.set_postfix({
                'loss': f"{loss_dict['loss'].item():.4f}",
                'task': f"{loss_dict['loss_task']:.4f}",
                'kd': f"{loss_dict['loss_kd']:.4f}"
            })
        
        # 计算平均损失
        for key in total_losses:
            total_losses[key] /= len(self.train_loader)
        
        train_f1 = f1_score(all_labels, all_preds, average='macro')
        
        return total_losses, train_f1
    
    def evaluate(self):
        """在验证集上评估"""
        self.student.eval()
        all_preds = []
        all_labels = []
        
        with torch.no_grad():
            for batch in tqdm(self.dev_loader, desc="Evaluating"):
                batch_data = {
                    'input_ids': batch['input_ids'].to(self.device),
                    'attention_mask': batch['attention_mask'].to(self.device),
                    'speaker_ids': batch['speaker_ids'].to(self.device),
                    'turn_ids': batch['turn_ids'].to(self.device)
                }
                labels = batch['labels'].to(self.device)
                
                outputs = self.student(**batch_data)
                preds = torch.argmax(outputs['logits'], dim=-1)
                
                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
        
        macro_f1 = f1_score(all_labels, all_preds, average='macro')
        
        return macro_f1
    
    def train(self):
        """完整蒸馏训练流程"""
        import os
        os.makedirs(self.save_dir, exist_ok=True)
        
        for epoch in range(self.num_epochs):
            print(f"\n{'='*60}")
            print(f"Distillation Epoch {epoch + 1}/{self.num_epochs}")
            print(f"{'='*60}")
            
            # 训练
            losses, train_f1 = self.train_epoch()
            print(f"Train - Total Loss: {losses['loss']:.4f}, "
                  f"Task: {losses['loss_task']:.4f}, "
                  f"KD: {losses['loss_kd']:.4f}, "
                  f"Hidden: {losses['loss_hidden']:.4f}, "
                  f"Attn: {losses['loss_attn']:.4f}, "
                  f"F1: {train_f1:.4f}")
            
            # 评估
            dev_f1 = self.evaluate()
            print(f"Dev - Macro-F1: {dev_f1:.4f}")
            
            # 保存最佳模型
            if dev_f1 > self.best_f1:
                self.best_f1 = dev_f1
                save_path = os.path.join(self.save_dir, 'best_distilled_model.pt')
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': self.student.state_dict(),
                    'projection_state_dict': self.hidden_projection.state_dict() if self.hidden_projection else None,
                    'dev_f1': dev_f1,
                }, save_path)
                print(f"✓ Saved best student model with F1: {dev_f1:.4f}")
        
        print(f"\nDistillation completed. Best Dev F1: {self.best_f1:.4f}")


# 使用示例
def run_distillation(teacher_checkpoint_path):
    """执行知识蒸馏"""
    from transformers import BertTokenizer
    
    # 1. 准备数据
    tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
    train_loader, dev_loader, test_loader = create_dataloaders(
        train_path='data/train.json',
        dev_path='data/dev.json',
        test_path='data/test.json',
        tokenizer=tokenizer,
        batch_size=32
    )
    
    # 2. 加载Teacher模型
    teacher_model = MultiTurnDialogueClassifier(
        encoder_name='bert-base-chinese',
        num_labels=12
    )
    teacher_checkpoint = torch.load(teacher_checkpoint_path)
    teacher_model.load_state_dict(teacher_checkpoint['model_state_dict'])
    print(f"Loaded teacher model with Dev F1: {teacher_checkpoint['dev_f1']:.4f}")
    
    # 3. 初始化Student模型（TinyBERT）
    student_model = MultiTurnDialogueClassifier(
        encoder_name='huawei-noah/TinyBERT_General_6L_768D',  # 使用TinyBERT预训练权重
        num_labels=12
    )
    
    # 4. 蒸馏训练
    distiller = DistillationTrainer(
        teacher_model=teacher_model,
        student_model=student_model,
        train_loader=train_loader,
        dev_loader=dev_loader,
        device='cuda',
        lr=3e-5,
        num_epochs=10,
        lambda_task=1.0,
        lambda_kd=1.0,
        lambda_hidden=0.5,
        lambda_attn=0.25,
        temperature=3.0
    )
    
    distiller.train()
    
    return student_model

# 运行蒸馏
# student_model = run_distillation('checkpoints/teacher/best_model.pt')
```

### 6.3 蒸馏数据与流程

#### 6.3.1 数据使用
- **数据源**：使用**同一任务的训练集**（带标注对话数据）
- **Teacher输出**：logits、turn-level hidden states、attention maps
- **Label使用**：仅用于L_task，其他损失使用Teacher输出

#### 6.3.2 蒸馏流程
1. **准备阶段**
   - 加载训练好的Teacher模型（eval模式，冻结参数）
   - 初始化Student模型（TinyBERT 6层 + aggregator + head）
   - 创建维度对齐层（312 → 768）

2. **训练循环**
   - 构建batch：input_ids (B, T, L)
   - Teacher forward（无梯度）→ 获取logits、hidden、attention
   - Student forward → 获取logits、hidden、attention
   - 计算4项蒸馏损失
   - 反向传播（仅更新Student）
   - 更新参数和学习率

3. **评估与保存**
   - 每个epoch在Dev集评估Macro-F1
   - 保存F1最高的checkpoint

### 6.4 蒸馏的作用机制

| 对齐类型 | 学习内容 | 效果 |
|---------|---------|------|
| **Logits** | Teacher的决策边界和类别关系 | 提升分类准确率 |
| **Hidden** | Teacher的语义表示空间 | 增强语义理解能力 |
| **Attention** | Teacher的注意力模式 | 学习关键信息定位 |
| **Task Loss** | 真实标签监督 | 保证任务性能 |

---

## 七、学生微调（Fine-tune after Distillation）

### 7.1 微调目的

蒸馏后的Student已经学到了Teacher的知识，但需要进一步优化以最大化下游任务性能。

**微调策略**：
- 移除所有蒸馏损失（KD、Hidden、Attention）
- 仅使用真实标签的交叉熵损失
- 使用极小的学习率进行精细调整

### 7.2 微调代码实现

```python
class StudentFinetuner:
    """
    Student模型微调器
    在蒸馏后进一步优化任务性能
    """
    def __init__(
        self,
        student_model: nn.Module,
        train_loader,
        dev_loader,
        device: str = 'cuda',
        lr: float = 5e-6,  # 极小的学习率
        num_epochs: int = 5,
        save_dir: str = 'checkpoints/student_finetuned'
    ):
        self.model = student_model.to(device)
        self.train_loader = train_loader
        self.dev_loader = dev_loader
        self.device = device
        self.num_epochs = num_epochs
        self.save_dir = save_dir
        
        # 优化器
        self.optimizer = AdamW(
            self.model.parameters(),
            lr=lr,
            weight_decay=0.01
        )
        
        # 学习率调度器
        total_steps = len(train_loader) * num_epochs
        warmup_steps = int(total_steps * 0.05)  # 5% warmup
        self.scheduler = get_linear_schedule_with_warmup(
            self.optimizer, warmup_steps, total_steps
        )
        
        # 损失函数
        self.criterion = nn.CrossEntropyLoss()
        
        # 早停
        self.best_f1 = 0.0
        self.patience = 3
        self.patience_counter = 0
        
        print(f"Fine-tuning with LR: {lr}")
        print(f"Total steps: {total_steps}, Warmup: {warmup_steps}")
    
    def train_epoch(self):
        """训练一个epoch"""
        self.model.train()
        total_loss = 0
        all_preds = []
        all_labels = []
        
        pbar = tqdm(self.train_loader, desc="Fine-tuning")
        for batch in pbar:
            input_ids = batch['input_ids'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)
            speaker_ids = batch['speaker_ids'].to(self.device)
            turn_ids = batch['turn_ids'].to(self.device)
            labels = batch['labels'].to(self.device)
            
            # 前向传播
            outputs = self.model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                speaker_ids=speaker_ids,
                turn_ids=turn_ids
            )
            
            logits = outputs['logits']
            loss = self.criterion(logits, labels)
            
            # 反向传播
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
            self.optimizer.step()
            self.scheduler.step()
            
            # 统计
            total_loss += loss.item()
            preds = torch.argmax(logits, dim=-1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            
            pbar.set_postfix({'loss': f'{loss.item():.4f}'})
        
        avg_loss = total_loss / len(self.train_loader)
        train_f1 = f1_score(all_labels, all_preds, average='macro')
        
        return avg_loss, train_f1
    
    def evaluate(self):
        """评估"""
        self.model.eval()
        all_preds = []
        all_labels = []
        
        with torch.no_grad():
            for batch in tqdm(self.dev_loader, desc="Evaluating"):
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                speaker_ids = batch['speaker_ids'].to(self.device)
                turn_ids = batch['turn_ids'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    speaker_ids=speaker_ids,
                    turn_ids=turn_ids
                )
                
                preds = torch.argmax(outputs['logits'], dim=-1)
                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
        
        macro_f1 = f1_score(all_labels, all_preds, average='macro')
        return macro_f1
    
    def train(self):
        """完整微调流程（带早停）"""
        import os
        os.makedirs(self.save_dir, exist_ok=True)
        
        for epoch in range(self.num_epochs):
            print(f"\n{'='*50}")
            print(f"Fine-tuning Epoch {epoch + 1}/{self.num_epochs}")
            print(f"{'='*50}")
            
            # 训练
            train_loss, train_f1 = self.train_epoch()
            print(f"Train Loss: {train_loss:.4f}, F1: {train_f1:.4f}")
            
            # 评估
            dev_f1 = self.evaluate()
            print(f"Dev Macro-F1: {dev_f1:.4f}")
            
            # 保存最佳模型
            if dev_f1 > self.best_f1:
                self.best_f1 = dev_f1
                self.patience_counter = 0
                
                save_path = os.path.join(self.save_dir, 'best_finetuned_model.pt')
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': self.model.state_dict(),
                    'dev_f1': dev_f1,
                }, save_path)
                print(f"✓ Saved best model with F1: {dev_f1:.4f}")
            else:
                self.patience_counter += 1
                print(f"No improvement. Patience: {self.patience_counter}/{self.patience}")
                
                if self.patience_counter >= self.patience:
                    print("Early stopping triggered!")
                    break
        
        print(f"\nFine-tuning completed. Best Dev F1: {self.best_f1:.4f}")


# 使用示例
def finetune_student(distilled_checkpoint_path):
    """微调蒸馏后的Student"""
    from transformers import BertTokenizer
    
    # 1. 准备数据
    tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
    train_loader, dev_loader, test_loader = create_dataloaders(
        train_path='data/train.json',
        dev_path='data/dev.json',
        test_path='data/test.json',
        tokenizer=tokenizer,
        batch_size=32
    )
    
    # 2. 加载蒸馏后的Student模型
    student_model = MultiTurnDialogueClassifier(
        encoder_name='huawei-noah/TinyBERT_General_6L_768D',
        num_labels=12
    )
    checkpoint = torch.load(distilled_checkpoint_path)
    student_model.load_state_dict(checkpoint['model_state_dict'])
    print(f"Loaded distilled model with Dev F1: {checkpoint['dev_f1']:.4f}")
    
    # 3. 微调
    finetuner = StudentFinetuner(
        student_model=student_model,
        train_loader=train_loader,
        dev_loader=dev_loader,
        device='cuda',
        lr=5e-6,
        num_epochs=5
    )
    
    finetuner.train()
    
    return student_model

# 运行微调
# final_model = finetune_student('checkpoints/student/best_distilled_model.pt')
```

### 7.3 微调超参数

| 参数 | 值 | 说明 |
|------|-----|------|
| **学习率** | 5e-6 | 极小，避免破坏蒸馏学到的知识 |
| **Epochs** | 3-5 | 早停防止过拟合 |
| **Patience** | 3 | 3个epoch无提升则停止 |
| **Warmup** | 5% | 少量warmup |
| **损失函数** | CrossEntropy | 仅使用真实标签 |

---

## 八、评估与误差分析

### 8.1 评估指标

#### 8.1.1 主要指标
- **Macro-F1**: 各类别F1的平均值，适合类别不平衡场景
- **Accuracy**: 整体准确率
- **Weighted-F1**: 按样本数加权的F1

#### 8.1.2 详细评估代码

```python
import numpy as np
from sklearn.metrics import (
    f1_score, accuracy_score, precision_recall_fscore_support,
    classification_report, confusion_matrix
)
import seaborn as sns
import matplotlib.pyplot as plt

class ModelEvaluator:
    """
    模型评估器
    提供全面的评估指标和可视化
    """
    def __init__(self, model, test_loader, device='cuda', label_names=None):
        self.model = model.to(device)
        self.test_loader = test_loader
        self.device = device
        self.label_names = label_names or [f"Class {i}" for i in range(12)]
    
    def evaluate(self):
        """全面评估"""
        self.model.eval()
        all_preds = []
        all_labels = []
        all_probs = []
        
        with torch.no_grad():
            for batch in tqdm(self.test_loader, desc="Evaluating"):
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                speaker_ids = batch['speaker_ids'].to(self.device)
                turn_ids = batch['turn_ids'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    speaker_ids=speaker_ids,
                    turn_ids=turn_ids
                )
                
                logits = outputs['logits']
                probs = F.softmax(logits, dim=-1)
                preds = torch.argmax(logits, dim=-1)
                
                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
                all_probs.extend(probs.cpu().numpy())
        
        all_preds = np.array(all_preds)
        all_labels = np.array(all_labels)
        all_probs = np.array(all_probs)
        
        # 计算指标
        metrics = self._compute_metrics(all_labels, all_preds, all_probs)
        
        # 打印报告
        self._print_report(metrics)
        
        # 绘制混淆矩阵
        self._plot_confusion_matrix(all_labels, all_preds)
        
        # 错误分析
        self._analyze_errors(all_labels, all_preds, all_probs)
        
        return metrics
    
    def _compute_metrics(self, labels, preds, probs):
        """计算评估指标"""
        metrics = {}
        
        # 整体指标
        metrics['accuracy'] = accuracy_score(labels, preds)
        metrics['macro_f1'] = f1_score(labels, preds, average='macro')
        metrics['weighted_f1'] = f1_score(labels, preds, average='weighted')
        
        # 每类指标
        precision, recall, f1, support = precision_recall_fscore_support(
            labels, preds, average=None
        )
        
        metrics['per_class'] = {
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'support': support
        }
        
        # 混淆矩阵
        metrics['confusion_matrix'] = confusion_matrix(labels, preds)
        
        return metrics
    
    def _print_report(self, metrics):
        """打印评估报告"""
        print("\n" + "="*70)
        print("                    EVALUATION REPORT")
        print("="*70)
        
        print(f"\n【整体指标】")
        print(f"  Accuracy    : {metrics['accuracy']:.4f}")
        print(f"  Macro-F1    : {metrics['macro_f1']:.4f}")
        print(f"  Weighted-F1 : {metrics['weighted_f1']:.4f}")
        
        print(f"\n【每类指标】")
        print(f"{'Class':<15} {'Precision':>10} {'Recall':>10} {'F1-Score':>10} {'Support':>10}")
        print("-"*60)
        
        for i in range(len(self.label_names)):
            print(f"{self.label_names[i]:<15} "
                  f"{metrics['per_class']['precision'][i]:>10.4f} "
                  f"{metrics['per_class']['recall'][i]:>10.4f} "
                  f"{metrics['per_class']['f1'][i]:>10.4f} "
                  f"{int(metrics['per_class']['support'][i]):>10}")
        
        print("="*70)
    
    def _plot_confusion_matrix(self, labels, preds, save_path='confusion_matrix.png'):
        """绘制混淆矩阵"""
        cm = confusion_matrix(labels, preds)
        
        plt.figure(figsize=(12, 10))
        sns.heatmap(
            cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=self.label_names,
            yticklabels=self.label_names
        )
        plt.xlabel('Predicted Label')
        plt.ylabel('True Label')
        plt.title('Confusion Matrix')
        plt.tight_layout()
        plt.savefig(save_path, dpi=300)
        print(f"\n✓ Confusion matrix saved to {save_path}")
    
    def _analyze_errors(self, labels, preds, probs, top_k=20):
        """错误样本分析"""
        errors = labels != preds
        error_indices = np.where(errors)[0]
        
        print(f"\n【错误分析】")
        print(f"  Total errors: {len(error_indices)} / {len(labels)} "
              f"({len(error_indices)/len(labels)*100:.2f}%)")
        
        # 找出置信度最高的错误样本
        error_probs = probs[error_indices]
        error_confidence = np.max(error_probs, axis=1)
        
        top_confident_errors = np.argsort(-error_confidence)[:top_k]
        
        print(f"\n  Top {min(top_k, len(top_confident_errors))} most confident errors:")
        print(f"  {'Index':<8} {'True':>8} {'Pred':>8} {'Confidence':>12}")
        print("  " + "-"*40)
        
        for idx in top_confident_errors:
            orig_idx = error_indices[idx]
            true_label = labels[orig_idx]
            pred_label = preds[orig_idx]
            confidence = error_confidence[idx]
            
            print(f"  {orig_idx:<8} {true_label:>8} {pred_label:>8} {confidence:>12.4f}")


# 使用示例
def comprehensive_evaluation(model_path, label_names=None):
    """完整评估流程"""
    from transformers import BertTokenizer
    
    # 准备数据
    tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
    _, _, test_loader = create_dataloaders(
        train_path='data/train.json',
        dev_path='data/dev.json',
        test_path='data/test.json',
        tokenizer=tokenizer,
        batch_size=32
    )
    
    # 加载模型
    model = MultiTurnDialogueClassifier(
        encoder_name='huawei-noah/TinyBERT_General_6L_768D',
        num_labels=12
    )
    checkpoint = torch.load(model_path)
    model.load_state_dict(checkpoint['model_state_dict'])
    print(f"Loaded model with Dev F1: {checkpoint.get('dev_f1', 'N/A')}")
    
    # 评估
    evaluator = ModelEvaluator(model, test_loader, label_names=label_names)
    metrics = evaluator.evaluate()
    
    return metrics

# 运行评估
# label_names = ['闲聊', '天气查询', '提醒设置', ...]  # 12个类别名称
# metrics = comprehensive_evaluation('checkpoints/student_finetuned/best_finetuned_model.pt', label_names)
```

### 8.2 模型对比

```python
def compare_models(teacher_path, student_distilled_path, student_finetuned_path):
    """对比Teacher和Student性能"""
    results = {}
    
    # 评估Teacher
    print("\n" + "="*70)
    print("EVALUATING TEACHER MODEL")
    print("="*70)
    results['teacher'] = comprehensive_evaluation(teacher_path)
    
    # 评估蒸馏后的Student
    print("\n" + "="*70)
    print("EVALUATING DISTILLED STUDENT")
    print("="*70)
    results['student_distilled'] = comprehensive_evaluation(student_distilled_path)
    
    # 评估微调后的Student
    print("\n" + "="*70)
    print("EVALUATING FINETUNED STUDENT")
    print("="*70)
    results['student_finetuned'] = comprehensive_evaluation(student_finetuned_path)
    
    # 性能对比
    print("\n" + "="*70)
    print("PERFORMANCE COMPARISON")
    print("="*70)
    print(f"{'Model':<25} {'Accuracy':>12} {'Macro-F1':>12} {'Weighted-F1':>12}")
    print("-"*70)
    
    for name, metrics in results.items():
        print(f"{name:<25} "
              f"{metrics['accuracy']:>12.4f} "
              f"{metrics['macro_f1']:>12.4f} "
              f"{metrics['weighted_f1']:>12.4f}")
    
    return results
```

---

## 九、导出与部署

### 9.1 ONNX导出

```python
import torch
import onnx
import onnxruntime as ort

def export_to_onnx(
    model_path: str,
    output_path: str = 'model.onnx',
    batch_size: int = 1,
    num_turns: int = 4,
    seq_length: int = 80
):
    """
    导出模型为ONNX格式
    """
    # 加载模型
    model = MultiTurnDialogueClassifier(
        encoder_name='huawei-noah/TinyBERT_General_6L_768D',
        num_labels=12
    )
    checkpoint = torch.load(model_path, map_location='cpu')
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    # 创建示例输入
    dummy_input = {
        'input_ids': torch.randint(0, 21128, (batch_size, num_turns, seq_length)),
        'attention_mask': torch.ones((batch_size, num_turns, seq_length), dtype=torch.long),
        'speaker_ids': torch.zeros((batch_size, num_turns), dtype=torch.long),
        'turn_ids': torch.arange(num_turns).unsqueeze(0).expand(batch_size, -1)
    }
    
    # 导出ONNX
    with torch.no_grad():
        torch.onnx.export(
            model,
            (
                dummy_input['input_ids'],
                dummy_input['attention_mask'],
                dummy_input['speaker_ids'],
                dummy_input['turn_ids']
            ),
            output_path,
            input_names=['input_ids', 'attention_mask', 'speaker_ids', 'turn_ids'],
            output_names=['logits'],
            dynamic_axes={
                'input_ids': {0: 'batch_size'},
                'attention_mask': {0: 'batch_size'},
                'speaker_ids': {0: 'batch_size'},
                'turn_ids': {0: 'batch_size'},
                'logits': {0: 'batch_size'}
            },
            opset_version=14,
            do_constant_folding=True
        )
    
    # 验证ONNX模型
    onnx_model = onnx.load(output_path)
    onnx.checker.check_model(onnx_model)
    
    print(f"✓ Model exported to {output_path}")
    
    # 测试ONNX Runtime
    ort_session = ort.InferenceSession(output_path)
    
    # 对比PyTorch和ONNX输出
    with torch.no_grad():
        torch_out = model(**dummy_input)
        torch_logits = torch_out['logits'].numpy()
    
    ort_inputs = {
        'input_ids': dummy_input['input_ids'].numpy(),
        'attention_mask': dummy_input['attention_mask'].numpy(),
        'speaker_ids': dummy_input['speaker_ids'].numpy(),
        'turn_ids': dummy_input['turn_ids'].numpy()
    }
    ort_logits = ort_session.run(None, ort_inputs)[0]
    
    # 检查差异
    diff = np.abs(torch_logits - ort_logits).max()
    print(f"  Max difference between PyTorch and ONNX: {diff:.6f}")
    
    if diff < 1e-4:
        print("  ✓ ONNX model verified successfully!")
    else:
        print(f"  ⚠ Warning: Large difference detected: {diff}")
    
    return ort_session


# 使用示例
# ort_session = export_to_onnx('checkpoints/student_finetuned/best_finetuned_model.pt')
```

### 9.2 模型量化

```python
from onnxruntime.quantization import quantize_dynamic, QuantType

def quantize_model(
    onnx_model_path: str,
    quantized_model_path: str = 'model_quantized.onnx'
):
    """
    INT8动态量化
    """
    quantize_dynamic(
        model_input=onnx_model_path,
        model_output=quantized_model_path,
        weight_type=QuantType.QInt8,
        optimize_model=True
    )
    
    # 对比模型大小
    import os
    orig_size = os.path.getsize(onnx_model_path) / 1024 / 1024
    quant_size = os.path.getsize(quantized_model_path) / 1024 / 1024
    
    print(f"✓ Model quantized successfully!")
    print(f"  Original size: {orig_size:.2f} MB")
    print(f"  Quantized size: {quant_size:.2f} MB")
    print(f"  Compression ratio: {orig_size/quant_size:.2f}x")
    
    return quantized_model_path

# 使用示例
# quantized_path = quantize_model('model.onnx')
```

### 9.3 FastAPI服务部署

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List
import onnxruntime as ort
import numpy as np
from transformers import BertTokenizer

# 初始化
app = FastAPI(title="TinyBERT Intent Classification API")
tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
ort_session = ort.InferenceSession('model_quantized.onnx')

# 意图类别映射
INTENT_LABELS = [
    "闲聊", "天气查询", "提醒设置", "音乐播放",
    "新闻阅读", "导航", "设备控制", "视觉问答",
    "空间事务", "知识问答", "任务规划", "其他"
]

class Turn(BaseModel):
    speaker: str  # "user" or "system"
    text: str

class DialogueRequest(BaseModel):
    turns: List[Turn]

class DialogueResponse(BaseModel):
    intent: str
    intent_id: int
    confidence: float
    all_probs: dict


@app.post("/predict", response_model=DialogueResponse)
def predict_intent(request: DialogueRequest):
    """
    预测多轮对话的意图
    """
    try:
        # 数据预处理
        input_data = preprocess_dialogue(request.turns)
        
        # ONNX推理
        ort_inputs = {
            'input_ids': input_data['input_ids'],
            'attention_mask': input_data['attention_mask'],
            'speaker_ids': input_data['speaker_ids'],
            'turn_ids': input_data['turn_ids']
        }
        
        logits = ort_session.run(None, ort_inputs)[0]
        probs = softmax(logits[0])
        
        # 获取预测结果
        intent_id = int(np.argmax(probs))
        intent = INTENT_LABELS[intent_id]
        confidence = float(probs[intent_id])
        
        # 所有类别概率
        all_probs = {INTENT_LABELS[i]: float(probs[i]) for i in range(len(INTENT_LABELS))}
        
        return DialogueResponse(
            intent=intent,
            intent_id=intent_id,
            confidence=confidence,
            all_probs=all_probs
        )
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


def preprocess_dialogue(turns: List[Turn], max_turns=4, max_seq_length=80):
    """预处理对话数据"""
    speaker_map = {'user': 0, 'system': 1}
    
    input_ids_list = []
    attention_mask_list = []
    speaker_ids_list = []
    
    for turn in turns[:max_turns]:
        encoded = tokenizer(
            turn.text,
            max_length=max_seq_length,
            padding='max_length',
            truncation=True,
            return_tensors='np'
        )
        
        input_ids_list.append(encoded['input_ids'][0])
        attention_mask_list.append(encoded['attention_mask'][0])
        speaker_ids_list.append(speaker_map.get(turn.speaker.lower(), 0))
    
    # Padding到max_turns
    while len(input_ids_list) < max_turns:
        input_ids_list.append(np.zeros(max_seq_length, dtype=np.int64))
        attention_mask_list.append(np.zeros(max_seq_length, dtype=np.int64))
        speaker_ids_list.append(0)
    
    return {
        'input_ids': np.array([input_ids_list], dtype=np.int64),
        'attention_mask': np.array([attention_mask_list], dtype=np.int64),
        'speaker_ids': np.array([speaker_ids_list], dtype=np.int64),
        'turn_ids': np.array([list(range(max_turns))], dtype=np.int64)
    }


def softmax(x):
    """Softmax函数"""
    e_x = np.exp(x - np.max(x))
    return e_x / e_x.sum()


@app.get("/health")
def health_check():
    """健康检查"""
    return {"status": "healthy", "model": "TinyBERT-Intent-Classifier"}


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### 9.4 客户端调用示例

```python
import requests
import json

def call_api(turns):
    """调用API"""
    url = "http://localhost:8000/predict"
    
    payload = {
        "turns": turns
    }
    
    response = requests.post(url, json=payload)
    
    if response.status_code == 200:
        result = response.json()
        print(f"Intent: {result['intent']}")
        print(f"Confidence: {result['confidence']:.4f}")
        print("\nAll probabilities:")
        for intent, prob in sorted(result['all_probs'].items(), key=lambda x: x[1], reverse=True):
            print(f"  {intent}: {prob:.4f}")
    else:
        print(f"Error: {response.status_code}")
        print(response.text)

# 使用示例
turns = [
    {"speaker": "user", "text": "今天下午提醒我喝水"},
    {"speaker": "system", "text": "好的，已设置提醒"},
    {"speaker": "user", "text": "取消所有提醒"},
    {"speaker": "system", "text": "已取消"}
]

call_api(turns)
```

### 9.5 Docker部署

```dockerfile
# Dockerfile
FROM python:3.9-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY model_quantized.onnx .
COPY api.py .

EXPOSE 8000

CMD ["uvicorn", "api:app", "--host", "0.0.0.0", "--port", "8000"]
```

```bash
# 构建和运行
docker build -t tinybert-intent-api .
docker run -d -p 8000:8000 tinybert-intent-api
```

---

## 十、完整训练流程

### 10.1 端到端训练脚本

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
TinyBERT多轮对话意图识别 - 完整训练流程
包含：数据加载、Teacher训练、蒸馏、微调、评估、导出
"""

import os
import torch
from transformers import BertTokenizer

# ============================================================================
# 配置参数
# ============================================================================

class Config:
    # 数据路径
    train_path = 'data/train.json'
    dev_path = 'data/dev.json'
    test_path = 'data/test.json'
    
    # 模型配置
    teacher_model_name = 'bert-base-chinese'
    student_model_name = 'huawei-noah/TinyBERT_General_6L_768D'
    num_labels = 12
    max_turns = 4
    max_seq_length = 80
    
    # Teacher训练
    teacher_lr = 2e-5
    teacher_epochs = 5
    teacher_batch_size = 32
    
    # 蒸馏训练
    distill_lr = 3e-5
    distill_epochs = 10
    distill_batch_size = 32
    lambda_task = 1.0
    lambda_kd = 1.0
    lambda_hidden = 0.5
    lambda_attn = 0.25
    temperature = 3.0
    
    # 微调
    finetune_lr = 5e-6
    finetune_epochs = 5
    finetune_batch_size = 32
    
    # 其他
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    seed = 42
    
    # 保存路径
    teacher_save_dir = 'checkpoints/teacher'
    student_distill_dir = 'checkpoints/student_distilled'
    student_finetune_dir = 'checkpoints/student_finetuned'
    onnx_save_path = 'model.onnx'
    quantized_save_path = 'model_quantized.onnx'


def set_seed(seed):
    """设置随机种子"""
    import random
    import numpy as np
    
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


# ============================================================================
# 主流程
# ============================================================================

def main():
    """完整训练流程"""
    cfg = Config()
    set_seed(cfg.seed)
    
    print("="*80)
    print(" TinyBERT多轮对话意图识别 - 完整训练流程")
    print("="*80)
    
    # ===== 步骤1: 准备数据 =====
    print("\n【步骤1/7】准备数据...")
    tokenizer = BertTokenizer.from_pretrained(cfg.teacher_model_name)
    
    train_loader, dev_loader, test_loader = create_dataloaders(
        train_path=cfg.train_path,
        dev_path=cfg.dev_path,
        test_path=cfg.test_path,
        tokenizer=tokenizer,
        batch_size=cfg.teacher_batch_size,
        max_turns=cfg.max_turns,
        max_seq_length=cfg.max_seq_length
    )
    
    print(f"✓ 数据加载完成")
    print(f"  训练集: {len(train_loader.dataset)} 样本")
    print(f"  验证集: {len(dev_loader.dataset)} 样本")
    print(f"  测试集: {len(test_loader.dataset)} 样本")
    
    # ===== 步骤2: 训练Teacher模型 =====
    print("\n【步骤2/7】训练Teacher模型 (BERT-base)...")
    
    teacher_checkpoint = os.path.join(cfg.teacher_save_dir, 'best_model.pt')
    
    if os.path.exists(teacher_checkpoint):
        print(f"  发现已有Teacher模型: {teacher_checkpoint}")
        print("  跳过训练，直接加载")
    else:
        teacher_model = MultiTurnDialogueClassifier(
            encoder_name=cfg.teacher_model_name,
            num_labels=cfg.num_labels,
            num_turns=cfg.max_turns,
            max_seq_length=cfg.max_seq_length
        )
        
        teacher_trainer = TeacherTrainer(
            model=teacher_model,
            train_loader=train_loader,
            dev_loader=dev_loader,
            device=cfg.device,
            lr=cfg.teacher_lr,
            num_epochs=cfg.teacher_epochs,
            save_dir=cfg.teacher_save_dir
        )
        
        teacher_trainer.train()
        print(f"✓ Teacher训练完成，最佳F1: {teacher_trainer.best_f1:.4f}")
    
    # ===== 步骤3: 加载Teacher模型 =====
    print("\n【步骤3/7】加载训练好的Teacher模型...")
    
    teacher_model = MultiTurnDialogueClassifier(
        encoder_name=cfg.teacher_model_name,
        num_labels=cfg.num_labels,
        num_turns=cfg.max_turns
    )
    teacher_ckpt = torch.load(teacher_checkpoint, map_location=cfg.device)
    teacher_model.load_state_dict(teacher_ckpt['model_state_dict'])
    teacher_model = teacher_model.to(cfg.device)
    
    print(f"✓ Teacher模型加载完成，Dev F1: {teacher_ckpt['dev_f1']:.4f}")
    
    # ===== 步骤4: 知识蒸馏 =====
    print("\n【步骤4/7】知识蒸馏 (Teacher → Student)...")
    
    student_distill_checkpoint = os.path.join(cfg.student_distill_dir, 'best_distilled_model.pt')
    
    if os.path.exists(student_distill_checkpoint):
        print(f"  发现已有蒸馏模型: {student_distill_checkpoint}")
        print("  跳过蒸馏，直接加载")
    else:
        student_model = MultiTurnDialogueClassifier(
            encoder_name=cfg.student_model_name,
            num_labels=cfg.num_labels,
            num_turns=cfg.max_turns
        )
        
        distiller = DistillationTrainer(
            teacher_model=teacher_model,
            student_model=student_model,
            train_loader=train_loader,
            dev_loader=dev_loader,
            device=cfg.device,
            lr=cfg.distill_lr,
            num_epochs=cfg.distill_epochs,
            lambda_task=cfg.lambda_task,
            lambda_kd=cfg.lambda_kd,
            lambda_hidden=cfg.lambda_hidden,
            lambda_attn=cfg.lambda_attn,
            temperature=cfg.temperature,
            save_dir=cfg.student_distill_dir
        )
        
        distiller.train()
        print(f"✓ 蒸馏完成，最佳F1: {distiller.best_f1:.4f}")
    
    # ===== 步骤5: Student微调 =====
    print("\n【步骤5/7】Student微调...")
    
    student_finetune_checkpoint = os.path.join(cfg.student_finetune_dir, 'best_finetuned_model.pt')
    
    if os.path.exists(student_finetune_checkpoint):
        print(f"  发现已有微调模型: {student_finetune_checkpoint}")
        print("  跳过微调，直接使用")
    else:
        student_model = MultiTurnDialogueClassifier(
            encoder_name=cfg.student_model_name,
            num_labels=cfg.num_labels,
            num_turns=cfg.max_turns
        )
        student_ckpt = torch.load(student_distill_checkpoint, map_location=cfg.device)
        student_model.load_state_dict(student_ckpt['model_state_dict'])
        
        finetuner = StudentFinetuner(
            student_model=student_model,
            train_loader=train_loader,
            dev_loader=dev_loader,
            device=cfg.device,
            lr=cfg.finetune_lr,
            num_epochs=cfg.finetune_epochs,
            save_dir=cfg.student_finetune_dir
        )
        
        finetuner.train()
        print(f"✓ 微调完成，最佳F1: {finetuner.best_f1:.4f}")
    
    # ===== 步骤6: 测试集评估 =====
    print("\n【步骤6/7】在测试集上评估最终模型...")
    
    final_model = MultiTurnDialogueClassifier(
        encoder_name=cfg.student_model_name,
        num_labels=cfg.num_labels,
        num_turns=cfg.max_turns
    )
    final_ckpt = torch.load(student_finetune_checkpoint, map_location=cfg.device)
    final_model.load_state_dict(final_ckpt['model_state_dict'])
    
    evaluator = ModelEvaluator(final_model, test_loader, device=cfg.device)
    test_metrics = evaluator.evaluate()
    
    print(f"\n✓ 测试集评估完成")
    print(f"  Accuracy: {test_metrics['accuracy']:.4f}")
    print(f"  Macro-F1: {test_metrics['macro_f1']:.4f}")
    
    # ===== 步骤7: 导出ONNX =====
    print("\n【步骤7/7】导出ONNX模型...")
    
    if not os.path.exists(cfg.onnx_save_path):
        export_to_onnx(
            model_path=student_finetune_checkpoint,
            output_path=cfg.onnx_save_path,
            batch_size=1,
            num_turns=cfg.max_turns,
            seq_length=cfg.max_seq_length
        )
    
    # 量化
    if not os.path.exists(cfg.quantized_save_path):
        quantize_model(cfg.onnx_save_path, cfg.quantized_save_path)
    
    print("\n" + "="*80)
    print(" 🎉 所有步骤完成！")
    print("="*80)
    print(f"\n【最终结果】")
    print(f"  Teacher (BERT-base) Dev F1: {teacher_ckpt['dev_f1']:.4f}")
    print(f"  Student (TinyBERT) Test F1: {test_metrics['macro_f1']:.4f}")
    print(f"  模型大小压缩: ~7.5x")
    print(f"  推理速度提升: ~9.4x")
    print(f"\n【输出文件】")
    print(f"  PyTorch模型: {student_finetune_checkpoint}")
    print(f"  ONNX模型: {cfg.onnx_save_path}")
    print(f"  量化ONNX: {cfg.quantized_save_path}")
    print("\n【部署建议】")
    print(f"  运行FastAPI服务: python api.py")
    print(f"  或使用Docker: docker run -p 8000:8000 tinybert-intent-api")
    print("="*80)


if __name__ == "__main__":
    main()
```

### 10.2 Requirements文件

```text
# requirements.txt
torch>=1.13.0
transformers>=4.25.0
scikit-learn>=1.2.0
numpy>=1.23.0
tqdm>=4.64.0
onnx>=1.12.0
onnxruntime>=1.13.0
fastapi>=0.95.0
uvicorn>=0.21.0
pydantic>=1.10.0
seaborn>=0.12.0
matplotlib>=3.6.0
```

---

## 十一、超参数配置总结

### 11.1 Teacher训练

| 参数 | 值 | 说明 |
|------|-----|------|
| **模型** | bert-base-chinese | 12层，768维 |
| **学习率** | 2e-5 (encoder)<br>4e-5 (task层) | 差异化学习率 |
| **Batch Size** | 32 | GPU显存允许可增大 |
| **Epochs** | 5 | 早停策略 |
| **Warmup** | 10% | 前10%步数线性warmup |
| **Weight Decay** | 0.01 | L2正则化 |
| **梯度裁剪** | 1.0 | max_norm=1.0 |

### 11.2 蒸馏训练

| 参数 | 值 | 说明 |
|------|-----|------|
| **模型** | TinyBERT_General_6L_768D | 6层，312维 |
| **学习率** | 3e-5 | 比Teacher略高 |
| **Batch Size** | 32 | 与Teacher保持一致 |
| **Epochs** | 10 | 蒸馏需要更多epoch |
| **λ_task** | 1.0 | 任务损失权重 |
| **λ_kd** | 1.0 | Logits蒸馏权重 |
| **λ_hidden** | 0.5 | Hidden对齐权重 |
| **λ_attn** | 0.25 | Attention对齐权重 |
| **温度T** | 3.0 | 软标签平滑度 |

### 11.3 微调训练

| 参数 | 值 | 说明 |
|------|-----|------|
| **学习率** | 5e-6 | 极小，精细调整 |
| **Batch Size** | 32 | 保持一致 |
| **Epochs** | 3-5 | 早停（patience=3） |
| **Warmup** | 5% | 少量warmup |
| **损失函数** | CrossEntropy | 仅真实标签 |

### 11.4 性能对比

| 模型 | 参数量 | 推理速度 | Macro-F1 | 备注 |
|------|--------|----------|----------|------|
| **BERT-base** | 110M | 1x | ~0.90 | Teacher基线 |
| **TinyBERT-6L (蒸馏后)** | 14.5M | ~9.4x | ~0.86-0.88 | 保留95%+性能 |
| **TinyBERT-6L (微调后)** | 14.5M | ~9.4x | ~0.88-0.90 | 接近Teacher |

---

## 十二、风险与注意事项

### 12.1 模型训练风险

#### 风险1：Teacher性能不佳
**问题**：如果Teacher模型本身性能差，Student也无法学好  
**解决方案**：
- 确保Teacher在Dev集上F1 > 0.85
- 检查数据质量和标注一致性
- 尝试增加训练数据或数据增强

#### 风险2：蒸馏不收敛
**问题**：蒸馏损失不下降或Student性能远低于Teacher  
**解决方案**：
- 调整蒸馏权重（λ），降低λ_hidden和λ_attn
- 降低温度T（3.0 → 2.0）
- 增加学习率或延长训练epoch

#### 风险3：过拟合
**问题**：训练集F1高，但验证集F1低  
**解决方案**：
- 增加Dropout（0.1 → 0.2）
- 提前停止训练（早停策略）
- 数据增强（同义词替换、回译等）

### 12.2 工程实施风险

#### 风险4：Tokenizer不匹配
**问题**：Teacher和Student使用不同的tokenizer导致错误  
**解决方案**：
- **必须**共享相同的tokenizer
- 如果添加特殊token，两者都要添加
- 验证vocab大小一致

#### 风险5：层映射不合理
**问题**：Student层数与Teacher不匹配，attention对齐失败  
**解决方案**：
```python
# 6层Student → 12层Teacher的标准映射
layer_mapping = {0: 0, 1: 2, 2: 4, 3: 6, 4: 8, 5: 10}

# 4层Student → 12层Teacher
layer_mapping = {0: 0, 1: 3, 2: 6, 3: 9}
```

#### 风险6：显存不足
**问题**：训练时OOM（Out of Memory）  
**解决方案**：
- 减小Batch Size（32 → 16）
- 启用梯度累积
- 使用混合精度训练（FP16）
- 减少max_seq_length（80 → 64）

### 12.3 数据质量风险

#### 风险7：标签噪声
**问题**：标注错误导致模型学习错误模式  
**解决方案**：
- 人工抽查标注质量（随机抽查10%）
- 使用confusion matrix识别混淆类别
- 重新标注易混淆样本

#### 风险8：类别不平衡
**问题**：某些意图样本过少，模型偏向多数类  
**解决方案**：
- 使用Focal Loss或Class-Weighted Loss
- 数据增强生成少数类样本
- 过采样（SMOTE等）

### 12.4 部署风险

#### 风险9：ONNX导出失败
**问题**：PyTorch模型转ONNX时报错  
**解决方案**：
- 检查opset版本兼容性
- 简化模型结构（避免复杂控制流）
- 使用`torch.onnx.export`的`verbose=True`调试

#### 风险10：推理精度下降
**问题**：量化后精度显著下降  
**解决方案**：
- 使用校准数据集进行量化
- 尝试静态量化代替动态量化
- 仅量化部分层（保留关键层的FP32）

### 12.5 关键检查清单

训练前检查：
- [ ] 数据格式正确，无缺失值
- [ ] Tokenizer配置一致
- [ ] GPU显存充足
- [ ] 所有超参数已设置

训练中监控：
- [ ] Loss正常下降
- [ ] Dev F1稳定上升
- [ ] 没有NaN或Inf
- [ ] 梯度范数正常

训练后验证：
- [ ] Teacher Dev F1 > 0.85
- [ ] Student与Teacher差距 < 5%
- [ ] Confusion matrix无明显偏差
- [ ] ONNX输出与PyTorch一致（diff < 1e-4）

---

## 十三、总结

### 13.1 完整流程回顾

```
数据准备
   ↓
Teacher训练（BERT-base）
   ↓
知识蒸馏（Logits + Hidden + Attention）
   ↓
Student微调（任务优化）
   ↓
评估与分析
   ↓
ONNX导出 + 量化
   ↓
FastAPI部署
```

### 13.2 核心优势

1. **性能保持**：TinyBERT达到BERT-base 96%+的性能
2. **模型压缩**：参数量减少7.5倍（110M → 14.5M）
3. **推理加速**：速度提升9.4倍
4. **易于部署**：ONNX + 量化，支持CPU高效推理

### 13.3 适用场景

- ✅ 对话系统意图识别
- ✅ 实时响应要求高的场景
- ✅ 资源受限的边缘设备
- ✅ 需要批量处理的服务

### 13.4 后续优化方向

1. **模型层面**
   - 尝试4层TinyBERT（更小更快）
   - 探索其他蒸馏策略（如MiniLM、MobileBERT）
   - 多任务学习（意图+槽位填充）

2. **数据层面**
   - 主动学习标注更多数据
   - 对抗样本生成增强鲁棒性
   - 跨领域数据迁移

3. **工程层面**
   - TensorRT加速
   - 模型缓存与批处理优化
   - A/B测试与在线学习

---

**本文档提供了从原理到实现的完整方案，是可直接执行的TinyBERT多轮对话意图识别项目指南。**

