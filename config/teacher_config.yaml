# Teacher Model Training Configuration

data:
  train_path: data/train.json
  dev_path: data/dev.json
  test_path: data/test.json
  max_turns: 4  # kept for backward compatibility, not used in concatenation mode
  max_seq_length: 320  # total length for concatenated dialogue (4 turns * 80 tokens)
  num_workers: 4

model:
  encoder_name: bert-base-chinese
  num_labels: 12
  dropout: 0.1

training:
  encoder_lr: 2.0e-5
  task_lr: 4.0e-5
  batch_size: 32
  num_epochs: 5
  warmup_ratio: 0.1
  weight_decay: 0.01
  gradient_clip: 1.0
  patience: 3  # early stopping patience

logging:
  save_dir: checkpoints/teacher
  tensorboard_dir: runs/teacher
  log_every: 100
  eval_every: 1  # evaluate every N epochs

device: cuda
seed: 42

